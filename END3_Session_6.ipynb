{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END3 Session 6.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpratiek/END_NLP/blob/main/END3_Session_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDG7AwmXuxGa",
        "outputId": "6685e0fc-b1d0-4acb-b1b2-5b8738589822"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!wget http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n",
        "# !wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:03:58--  http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\n",
            "Resolving qim.fs.quoracdn.net (qim.fs.quoracdn.net)... 151.101.1.2, 151.101.65.2, 151.101.129.2, ...\n",
            "Connecting to qim.fs.quoracdn.net (qim.fs.quoracdn.net)|151.101.1.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58176133 (55M) [text/tab-separated-values]\n",
            "Saving to: ‘quora_duplicate_questions.tsv.1’\n",
            "\n",
            "quora_duplicate_que 100%[===================>]  55.48M   145MB/s    in 0.4s    \n",
            "\n",
            "2021-11-25 16:03:58 (145 MB/s) - ‘quora_duplicate_questions.tsv.1’ saved [58176133/58176133]\n",
            "\n",
            "unzip:  cannot find or open data.zip, data.zip.zip or data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AimrOHQaojVx"
      },
      "source": [
        "df = pd.read_csv(\"/content/quora_duplicate_questions.tsv\", sep='\\t')"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAKAyvyOAiBU"
      },
      "source": [
        "df = df[df['is_duplicate'] ==1]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAzWeFvcVw5B",
        "outputId": "806bb325-1ccd-4d65-a8dd-aee12f7f5098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
              "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>How can I be a good geologist?</td>\n",
              "      <td>What should I do to be a great geologist?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>24</td>\n",
              "      <td>How do I read and find my YouTube comments?</td>\n",
              "      <td>How can I see all my Youtube comments?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>What can make Physics easy to learn?</td>\n",
              "      <td>How can you make physics easy to learn?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>27</td>\n",
              "      <td>28</td>\n",
              "      <td>What was your first sexual experience like?</td>\n",
              "      <td>What was your first sexual experience?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404280</th>\n",
              "      <td>404280</td>\n",
              "      <td>537922</td>\n",
              "      <td>537923</td>\n",
              "      <td>What are some outfit ideas to wear to a frat p...</td>\n",
              "      <td>What are some outfit ideas wear to a frat them...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404281</th>\n",
              "      <td>404281</td>\n",
              "      <td>99131</td>\n",
              "      <td>81495</td>\n",
              "      <td>Why is Manaphy childish in Pokémon Ranger and ...</td>\n",
              "      <td>Why is Manaphy annoying in Pokemon ranger and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404282</th>\n",
              "      <td>404282</td>\n",
              "      <td>1931</td>\n",
              "      <td>16773</td>\n",
              "      <td>How does a long distance relationship work?</td>\n",
              "      <td>How are long distance relationships maintained?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404284</th>\n",
              "      <td>404284</td>\n",
              "      <td>537926</td>\n",
              "      <td>537927</td>\n",
              "      <td>What does Jainism say about homosexuality?</td>\n",
              "      <td>What does Jainism say about Gays and Homosexua...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404286</th>\n",
              "      <td>404286</td>\n",
              "      <td>18840</td>\n",
              "      <td>155606</td>\n",
              "      <td>Do you believe there is life after death?</td>\n",
              "      <td>Is it true that there is life after death?</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149263 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...  is_duplicate\n",
              "5            5  ...             1\n",
              "7            7  ...             1\n",
              "11          11  ...             1\n",
              "12          12  ...             1\n",
              "13          13  ...             1\n",
              "...        ...  ...           ...\n",
              "404280  404280  ...             1\n",
              "404281  404281  ...             1\n",
              "404282  404282  ...             1\n",
              "404284  404284  ...             1\n",
              "404286  404286  ...             1\n",
              "\n",
              "[149263 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azcpt1zTBMXe",
        "outputId": "4a487766-1ab4-4c05-911d-c9a1b2cb0042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df[\"question2\"].str.len().max()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "295"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8CW4Z71DCup"
      },
      "source": [
        "df['input'] = df[\"question1\"] + '\\t' + df[\"question2\"]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6z6M6OMV-u_",
        "outputId": "191b1db3-4c87-4fa7-baa9-75dbb35654db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df['input'][5].split(\"\\t\")"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
              " \"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7_sFmAVDX_l"
      },
      "source": [
        "li = df['input'].to_list()"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnS0VS0I0Ur1"
      },
      "source": [
        "with open('eng-eng.txt', 'w') as f:\n",
        "  for i in li:\n",
        "    f.write(i)\n",
        "    f.write('\\n')"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta2wZ3N6Pbsn"
      },
      "source": [
        ""
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAeTnRDIDzoV",
        "outputId": "d7de59d3-8257-431f-fc9a-1b2d4dcdd83e"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('/content/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "MAX_LENGTH = 295\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'eng', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 149263 sentence pairs\n",
            "Trimmed to 679 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 1506\n",
            "eng 1475\n",
            "['what are the best was to lose weight ?', 'i m fat . how do i lose weight ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIufD-65RX1r"
      },
      "source": [
        "# input_lang, output_lang, pairs = readLangs('eng', 'fra', reverse=False)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8K-0doYRj8H"
      },
      "source": [
        "# lines = open('/content/%s-%s.txt' % ('eng', 'eng'), encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "\n",
        "#     # Split every line into pairs and normalize\n",
        "# pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMK3-ahaTAVq"
      },
      "source": [
        "# for l in lines:\n",
        "#   print(l.split(\"\\t\"))\n",
        "#   break"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1FaOAEOHyUk",
        "outputId": "2c8743ff-7a3b-440f-e0f3-40054fb8cad3"
      },
      "source": [
        "type(pairs)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89oBpJAcIF0N",
        "outputId": "2afbeb80-67e7-4d48-8033-bd74623864e0"
      },
      "source": [
        "pairs[0:5]"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i am my weight is kg and my height is . ft . what should i do to gain height and weight ?',\n",
              "  'i m year old my height is feet inches and my weight is kg what should i do to gain weight ?'],\n",
              " ['what can i do when i m ?', 'i m . what should i do ?'],\n",
              " ['how is trump becoming the president affect the indians applying for an ms in the us mech ?',\n",
              "  'i am an indian planning to go to us for ms a stem course this january . if trump wins how will that affect my future in us ?'],\n",
              " ['kindly tell me whole process of admission at vits vellore for biotech .i m a bio student in . .i don t have math there ?',\n",
              "  'i m bio student how i got admission in vits vellore in biotechnology and i don t have math in ?'],\n",
              " ['what can i do to improve my english grammar ?',\n",
              "  'i am poor in english grammar so how should i improve my grammar ?']]"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwsY5FiIIK7A"
      },
      "source": [
        "# The architecture we are building\n",
        "\n",
        "![image](https://miro.medium.com/max/1838/1*tXchCn0hBSUau3WO0ViD7w.jpeg)\n",
        "\n",
        "As we can see here, we will have an encoder, an attention mechanism block and decoder. In the final code the attention mechanicm block and decoder will be merged into single block as we need both to work together. \n",
        "\n",
        "As we can see here, we need to create a copy of h1, h2, h3 and h4. These are encoder outputs for a sentence with 4 words. \n",
        "\n",
        "# Encoder\n",
        "\n",
        "We will build our encoder with a GRU, but that's all we know. Let's NOT strait away build a class, but see how to come up with one for the Encoder. We need to answer few questions first:\n",
        "1. what would be the hidden size of our GRU\n",
        "2. What would be the input size\n",
        "3. What would be the embedding dimesions. \n",
        "\n",
        "For simplicity, lets keep 1. and 3. to be 256. \n",
        "\n",
        "We can't feed our input directly to GRU, we need to tensorize it, convert to embeddings first. \n",
        "\n",
        "`embedding = nn.Embedding(input_size, hidden_size) `\n",
        "\n",
        "## What is input_size?\n",
        "\n",
        "Remember the line below?\n",
        "\n",
        "`input_lang, output_lang, pairs = prepareData('eng', 'fra', True)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY5ZRwOkIHiR",
        "outputId": "f2c95234-004b-4d62-f2e1-c9db4bbb9f8d"
      },
      "source": [
        "input_lang"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Lang at 0x7f81a7283b10>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCPsLCpTIRmz",
        "outputId": "51af0fe6-a835-447b-bb67-8286d2e08c00"
      },
      "source": [
        "help(input_lang)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Lang in module __main__ object:\n",
            "\n",
            "class Lang(builtins.object)\n",
            " |  Lang(name)\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, name)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  addSentence(self, sentence)\n",
            " |  \n",
            " |  addWord(self, word)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzMp8NcFIUlC",
        "outputId": "e577e2da-97cc-4045-fe5f-ae11e7c51aee"
      },
      "source": [
        "input_lang.__dict__.items()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('name', 'eng'), ('word2index', {'i': 2, 'am': 3, 'my': 4, 'weight': 5, 'is': 6, 'kg': 7, 'and': 8, 'height': 9, '.': 10, 'ft': 11, 'what': 12, 'should': 13, 'do': 14, 'to': 15, 'gain': 16, '?': 17, 'can': 18, 'when': 19, 'm': 20, 'how': 21, 'trump': 22, 'becoming': 23, 'the': 24, 'president': 25, 'affect': 26, 'indians': 27, 'applying': 28, 'for': 29, 'an': 30, 'ms': 31, 'in': 32, 'us': 33, 'mech': 34, 'kindly': 35, 'tell': 36, 'me': 37, 'whole': 38, 'process': 39, 'of': 40, 'admission': 41, 'at': 42, 'vits': 43, 'vellore': 44, 'biotech': 45, '.i': 46, 'a': 47, 'bio': 48, 'student': 49, 'don': 50, 't': 51, 'have': 52, 'math': 53, 'there': 54, 'improve': 55, 'english': 56, 'grammar': 57, 'easiest': 58, 'way': 59, 'earn': 60, 'money': 61, 'from': 62, 'online': 63, 'good': 64, 'shape': 65, 'but': 66, 'trouble': 67, 'spot': 68, 'cellulite': 69, 'right': 70, 'under': 71, 'glutes': 72, 'top': 73, 'thighs': 74, 'get': 75, 'rid': 76, 'this': 77, 'petite': 78, 'year': 79, 'old': 80, 'woman': 81, 'who': 82, 'looks': 83, 'very': 84, 'young': 85, 'despite': 86, 'years': 87, 'work': 88, 'experience': 89, 'it': 90, 's': 91, 'disadvantage': 92, 'profession': 93, 'are': 94, 'some': 95, 'ways': 96, 'look': 97, 'act': 98, 'older': 99, 'more': 100, 'authoritative': 101, 'be': 102, 'increased': 103, 'after': 104, 'or': 105, 'age': 106, 'too': 107, 'heavy': 108, 'as': 109, 'pair': 110, 'skater': 111, 'lb': 112, 'late': 113, '!': 114, 'which': 115, 'best': 116, 'institute': 117, 'lern': 118, 'fashion': 119, 'designing': 120, 'pune': 121, 'will': 122, 'give': 123, 'scope': 124, 'skill': 125, 'while': 126, 'woking': 127, 'love': 128, 'with': 129, 'classmate': 130, 'gay': 131, 'he': 132, 'straight': 133, 'if': 134, 'muslim': 135, 'country': 136, 'suck': 137, 'up': 138, 'lose': 139, 'could': 140, 'control': 141, 'emotions': 142, 'negative': 143, 'thoughts': 144, 'entering': 145, 'world': 146, 'video': 147, 'game': 148, 'programming': 149, 'want': 150, 'know': 151, 'language': 152, 'learn': 153, 'because': 154, 'so': 155, 'many': 156, 'languages': 157, 'not': 158, 'one': 159, 'start': 160, 'you': 161, 'recommend': 162, 'that': 163, 'easy': 164, 'used': 165, 'platforms': 166, 'use': 167, 'javascript': 168, 'competitive': 169, 'stock': 170, 'market': 171, 'teach': 172, 'myself': 173, 'basics': 174, 'build': 175, 'self': 176, 'esteem': 177, 'advice': 178, 'first': 179, 'time': 180, 'entrepreneur': 181, 'founder': 182, 'ceo': 183, 'on': 184, 'hires': 185, 'still': 186, 'college': 187, 'information': 188, 'security': 189, 'order': 190, 'those': 191, 'large': 192, 'firms': 193, 'like': 194, 'google': 195, 'oracle': 196, 'facebook': 197, 'need': 198, 'manage': 199, 'marks': 200, 'all': 201, 'sections': 202, 'cat': 203, 'taking': 204, 'into': 205, 'consideration': 206, 'much': 207, 'percentile': 208, 'now': 209, 'donald': 210, 'international': 211, 'students': 212, 'stop': 213, 'coming': 214, 'universities': 215, 'become': 216, 'successful': 217, 'politician': 218, 'girl': 219, 'overweight': 220, 'feet': 221, 'inches': 222, 'tall': 223, 'weigh': 224, 'pounds': 225, 'why': 226, 'even': 227, 'fair': 228, 'skinned': 229, 'women': 230, 'crotch': 231, 'inner': 232, 'near': 233, 'dark': 234, 'colour': 235, 'concentrate': 236, 'studies': 237, 'instead': 238, 'thinking': 239, 'about': 240, 'others': 241, 'shy': 242, 'unable': 243, 'talk': 244, 'any': 245, 'nervous': 246, 'freaked': 247, 'out': 248, 'around': 249, 'them': 250, 'solution': 251, 'buy': 252, 'redmi': 253, 'note': 254, 'come': 255, 'family': 256, 'friends': 257, 'friend': 258, 'already': 259, 'boyfriend': 260, 'she': 261, 'doesn': 262, 'going': 263, 'approach': 264, 'dream': 265, 'would': 266, 'increase': 267, 'his': 268, 'gym': 269, 'since': 270, 'months': 271, 'proteins': 272, 'cleaning': 273, 'face': 274, 'regularly': 275, 'having': 276, 'pustules': 277, 'cheeks': 278, 'homosexuals': 279, 'attracted': 280, 'themselves': 281, 'by': 282, 'looking': 283, 'mirror': 284, 'were': 285, 'united': 286, 'states': 287, 'places': 288, 'goa': 289, 'visit': 290, 'nowhere': 291, 're': 292, 'given': 293, 'million': 294, 'dollars': 295, 'post': 296, 'marriage': 297, 'fallen': 298, 'incredible': 299, 'whom': 300, 'known': 301, 'than': 302, 'problem': 303, 'has': 304, 'starting': 305, 'new': 306, 'job': 307, 'next': 308, 'week': 309, 'just': 310, 'received': 311, 'call': 312, 'interview': 313, 'recruiter': 314, 'situation': 315, 'b': 316, '.tech': 317, 'ece': 318, 'fresher': 319, 'electronics': 320, 'companies': 321, 'jaipur': 322, 'apply': 323, 'feel': 324, 'tired': 325, 'life': 326, 'invest': 327, 'lack': 328, 'knowledge': 329, 'share': 330, 'proceed': 331, 'guys': 332, 'depressed': 333, 'study': 334, 'medicine': 335, 'working': 336, 'quality': 337, 'smart': 338, 'textiles': 339, 'method': 340, 'play': 341, 'guitar': 342, 'confidence': 343, 'company': 344, 'provides': 345, 'seo': 346, 'services': 347, 'delhi': 348, 'alone': 349, 'things': 350, 'advise': 351, 'turn': 352, 'turning': 353, 'month': 354, 'india': 355, 'python': 356, 'may': 357, 'body': 358, 'platform': 359, 'chemical': 360, 'engineer': 361, 'offer': 362, 'canada': 363, 'join': 364, 'indian': 365, 'army': 366, 'days': 367, 'period': 368, 'pregnant': 369, 'code': 370, 'go': 371, 'worried': 372, 'future': 373, 'jumping': 374, 'skipping': 375, 'male': 376, 'iit': 377, 'jee': 378, 'aspirant': 379, 'concepts': 380, 'clear': 381, 'full': 382, 'physics': 383, 'organic': 384, 'well': 385, 'three': 386, 'digit': 387, 'rank': 388, 'laptop': 389, 'intel': 390, 'core': 391, 'quad': 392, 'processor': 393, 'gb': 394, 'ram': 395, 'tb': 396, 'hdd': 397, 'graphics': 398, 'memory': 399, 'hours': 400, 'battery': 401, 'backup': 402, 'course': 403, 'mechanical': 404, 'engineering': 405, 'suits': 406, 'we': 407, 'make': 408, 'being': 409, 'such': 410, 'pushover': 411, 'realistically': 412, 'efficiently': 413, 'possible': 414, 'run': 415, 'seconds': 416, 'cm': 417, 'yrs': 418, 'times': 419, 'jerk': 420, 'planning': 421, 'e': 422, 'commerce': 423, 'business': 424, 'register': 425, 'easily': 426, 'rd': 427, 'published': 428, 'journal': 429, 'ijera': 430, 'mini': 431, 'project': 432, 'research': 433, 'application': 434, 'cgpa': 435, 'th': 436, 'semester': 437, 'iisc': 438, 'few': 439, 'was': 440, 'let': 441, 'regrets': 442, 'slowly': 443, 'slipping': 444, 'depression': 445, 'deal': 446, 'weeks': 447, 'castor': 448, 'oil': 449, 'early': 450, 'delievery': 451, 'preparation': 452, 'ias': 453, 'exam': 454, 'waterproof': 455, 'dslr': 456, 'camera': 457, 'uk': 458, 'tier': 459, 'general': 460, 'visa': 461, 'mma': 462, 'fighter': 463, 'training': 464, 'skilled': 465, 'enough': 466, 'android': 467, 'app': 468, 'development': 469, 'machine': 470, 'learning': 471, 'where': 472, 'web': 473, 'clients': 474, 'hedge': 475, 'fund': 476, 'using': 477, 'internet': 478, 'quiet': 479, 'always': 480, 'been': 481, 'mostly': 482, 'something': 483, 'innovative': 484, 'zero': 485, 'level': 486, 'really': 487, 'java': 488, 'programmer': 489, 'goal': 490, 'cyber': 491, 'birthday': 492, 'today': 493, 'completing': 494, 'nd': 495, 'computer': 496, 'gate': 497, 'stream': 498, 'cs': 499, 'youtube': 500, 'gaming': 501, 'channel': 502, 'words': 503, 'encouragement': 504, 'helpful': 505, 'tips': 506, 'includingexperience': 507, 'nepalese': 508, '.sc': 509, 'calcutta': 510, 'university': 511, 'phd': 512, 'astrophysics': 513, 'iia': 514, 'banglore': 515, 'ncra': 516, 'tifr': 517, 'millionaire': 518, 'before': 519, 'bangalore': 520, 'day': 521, 'great': 522, 'name': 523, 'quora': 524, 'answer': 525, 'question': 526, 'anonymously': 527, 'celebrate': 528, 'diwali': 529, 'festival': 530, 'lights': 531, 'loose': 532, 'final': 533, 'electrical': 534, 'projects': 535, 'prepare': 536, 'campus': 537, 'placement': 538, 'soon': 539, 'home': 540, 'eligibility': 541, 'criteria': 542, 'doing': 543, 'germany': 544, 'professional': 545, 'gambling': 546, 'career': 547, 'reach': 548, 'billionaire': 549, 'status': 550, 'your': 551, 'feeling': 552, 'low': 553, 'cool': 554, 'school': 555, 'bag': 556, 'carry': 557, 'during': 558, 'think': 559, 'growth': 560, 'trip': 561, 'january': 562, 'stay': 563, 'limited': 564, 'budget': 565, 'rs': 566, 'grow': 567, 'no': 568, 'idea': 569, 'figure': 570, 'female': 571, 'neanderthal': 572, 'men': 573, 'suicidal': 574, 'footballer': 575, 'actually': 576, 'bad': 577, 'fighting': 578, 'someone': 579, 'employed': 580, 'parents': 581, 'directors': 582, 'venture': 583, 'their': 584, 'recommendation': 585, 'letters': 586, 'second': 587, 'graduation': 588, 'city': 589, 'return': 590, 'hometown': 591, 'reactions': 592, 'nervousness': 593, 'considering': 594, 'dumb': 595, 'shall': 596, 'lot': 597, 'mpstme': 598, 'nmims': 599, 'enrolled': 600, 'btech': 601, 'mumbai': 602, 'affiliated': 603, 'master': 604, 'eat': 605, 'k': 606, 'race': 607, 'appearing': 608, 'executive': 609, 'december': 610, 'books': 611, 'reading': 612, 'significantly': 613, 'medical': 614, 'cab': 615, 'uber': 616, 'ola': 617, 'profitable': 618, 'car': 619, 'purchase': 620, 'less': 621, 'feminine': 622, 'mutual': 623, 'funds': 624, 'plan': 625, 'here': 626, 'onwards': 627, 'find': 628, 'housewife': 629, 'resigned': 630, 'software': 631, 'opportunities': 632, 'option': 633, 'instrumentation': 634, 'sector': 635, 'graphic': 636, 'card': 637, 'hard': 638, 'disk': 639, '.which': 640, 'trying': 641, 'larger': 642, 'butt': 643, 'bodyweight': 644, 'exercises': 645, 'reps': 646, 'sets': 647, 'experienced': 648, 'field': 649, 'design': 650, 'courses': 651, 'help': 652, 'little': 653, 'preparing': 654, 'positive': 655, 'does': 656, 'take': 657, 'cargo': 658, 'plane': 659, 'pilot': 660, 'quickly': 661, 'studying': 662, 'class': 663, 'pcm': 664, 'entrance': 665, 'exams': 666, 'other': 667, 'options': 668, 'excluding': 669, 'joining': 670, 'mncs': 671, 'own': 672, 'explain': 673, 'concept': 674, 'model': 675, 'prospects': 676, 'better': 677, 'meet': 678, 'guy': 679, 'tummy': 680, 'fat': 681, 'idhtt': 682, 'slang': 683, '': 684, 'secured': 685, 'acrobat': 686, 'introduce': 687, 'olds': 688, 'child': 689, 'had': 690, 'supposed': 691, 'commercial': 692, 'living': 693, 'ideal': 694, 'diet': 695, 'follow': 696, 'overcome': 697, 'wedding': 698, 'never': 699, 'girlfriend': 700, 'martial': 701, 'arts': 702, 'anyone': 703, 'suggest': 704, 'must': 705, 'social': 706, 'junior': 707, 'high': 708, 'graduating': 709, 'particularly': 710, 'circuit': 711, 'area': 712, 'lazy': 713, 'sometimes': 714, 'change': 715, 'standard': 716, 'spend': 717, 'iphone': 718, 'rap': 719, 'artist': 720, 'inspiration': 721, 'write': 722, 'songs': 723, 'anymore': 724, 'boost': 725, 'detail': 726, 'developing': 727, 'websites': 728, 'presentation': 729, 'among': 730, 'mischievous': 731, 'group': 732, 'often': 733, 'finds': 734, 'fun': 735, 'everything': 736, 'lowers': 737, '.what': 738, 'mres': 739, 'biomedical': 740, 'science': 741, 'topic': 742, 'poster': 743, 'suggestions': 744, 'royal': 745, 'enfield': 746, 'thunderbird': 747, 'cc': 748, 'long': 749, 'tours': 750, 'similiar': 751, 'bikes': 752, 'realistic': 753, 'formula': 754, 'live': 755, 'fall': 756, 'gets': 757, 'elected': 758, 'download': 759, 'mobile': 760, 'phone': 761, 'interested': 762, 'construction': 763, 'ask': 764, 'technical': 765, 'doubts': 766, 'related': 767, 'begin': 768, 'delve': 769, 'consultancy': 770, 'bsc': 771, 'biz': 772, 'mgt': 773, 'continue': 774, 'msc': 775, 'fields': 776, 'md': 777, 'internship': 778, 'switzerland': 779, 'ibm': 780, 'held': 781, 'took': 782, 'test': 783, 'morning': 784, 'came': 785, 'back': 786, 've': 787, 'pregnancy': 788, 'symptoms': 789, 'head': 790, 'suffering': 791, 'exercise': 792, 'eating': 793, 'drinking': 794, 'significant': 795, 'amount': 796, 'currently': 797, 'various': 798, 'materials': 799, 'proper': 800, 'stuff': 801, 'miss': 802, 'micro': 803, 'sim': 804, 'adapter': 805, 'put': 806, 'nano': 807, 'samsung': 808, 'galaxy': 809, 'provider': 810, 'most': 811, 'essential': 812, 'report': 813, 'fault': 814, 'talktalk': 815, 'scored': 816, 'cbse': 817, 'term': 818, 'chance': 819, 'performing': 820, 'scoring': 821, 'joined': 822, 'fiitjee': 823, 'please': 824, 'reasons': 825, 'management': 826, 'anything': 827, 'exercising': 828, 'every': 829, 'seven': 830, 'loneliness': 831, 'unpredictably': 832, 'moody': 833, 'fix': 834, 'convince': 835, 'mom': 836, 'sign': 837, 'local': 838, 'drivers': 839, 'education': 840, 'total': 841, 'failure': 842, 'wasted': 843, 'absolutely': 844, 'nothing': 845, 'laziness': 846, '.can': 847, 'track': 848, 'doctor': 849, 'tata': 850, 'safari': 851, 'storme': 852, 'mahindra': 853, 'xuv': 854, 'goals': 855, 'colleges': 856, 'try': 857, 'citizen': 858, 'niece': 859, 'her': 860, 'green': 861, 'faster': 862, 'gsoc': 863, 'getting': 864, 'daughter': 865, 'suited': 866, 'iot': 867, 'nit': 868, 'through': 869, 'multi': 870, 'sad': 871, 'lower': 872, 'below': 873, 'transgender': 874, 'simulate': 875, 'kerala': 876, 'prove': 877, 'only': 878, 'person': 879, 'matrix': 880, 'everyone': 881, 'else': 882, 'program': 883, 'lic': 884, 'jeevan': 885, 'anand': 886, 'saral': 887, 'including': 888, 'experiences': 889, 'attempt': 890, 'cpt': 891, 'june': 892, 'syllabus': 893, 'applicable': 894, 'available': 895, 'pursue': 896, '.designing': 897, 'st': 898, 'part': 899, 'robocon': 900, 'team': 901, 'told': 902, 'crush': 903, 'wants': 904, 'these': 905, 'feelings': 906, 'legal': 907, 'date': 908, 'completion': 909, '.pharm': 910, 'letter': 911, 'word': 912, 'normally': 913, 'u': 914, 'remove': 915, 'll': 916, 'above': 917, 'cant': 918, 'see': 919, 'interesting': 920, 'solves': 921, 'limit': 922, 'also': 923, 'send': 924, 'grps': 925, 'enter': 926, 'countries': 927, 'valid': 928, 'decision': 929, 'making': 930, 'windows': 931, 'pro': 932, 'touchpad': 933, 'scrolled': 934, 'two': 935, 'finger': 936, '.how': 937, 'fixed': 938, 'senior': 939, 'dropout': 940, 'armed': 941, 'forces': 942, 'third': 943, 'upsc': 944, 'ibps': 945, 'bank': 946, 'po': 947, '.am': 948, 'pregnent': 949, 'hindu': 950, 'boy': 951, 'marry': 952, 'mother': 953, 'brother': 954, 'cse': 955, 'product': 956, 'based': 957, 'slim': 958, 'talking': 959, 'asking': 960, 'girls': 961, 'over': 962, 'digital': 963, 'marketing': 964, 'animes': 965, 'activities': 966, 'adults': 967, 'burnt': 968, 'fastest': 969, 'tricks': 970, 'introverts': 971, 'care': 972, 'aleppo': 973, 'coding': 974, 'skills': 975, 'though': 976, 'completed': 977, 'degree': 978, 'permanently': 979, 'viewing': 980, 'watching': 981, 'pornography': 982, 'masturbatory': 983, 'aid': 984, 'lost': 985, 'greater': 986, 'god': 987, 'evil': 988, 'devil': 989, 'poor': 990, 'rich': 991, 'die': 992, 'riddle': 993, 'server': 994, 'side': 995, 'affordable': 996, 'accommodation': 997, 'rental': 998, 'service': 999, 'zealand': 1000, 'earning': 1001, 'photography': 1002, 'tutorials': 1003, 'single': 1004, 'prone': 1005, 'accidents': 1006, 'minimum': 1007, 'insurance': 1008, 'required': 1009, 'state': 1010, 'absolute': 1011, 'rate': 1012, 'website': 1013, 'bike': 1014, 'lakhs': 1015, 'south': 1016, 'africa': 1017, 'ncr': 1018, 'optimize': 1019, 'underemployed': 1020, 'underpaid': 1021, 'facial': 1022, 'hair': 1023, 'chin': 1024, 'mustache': 1025, 'beard': 1026, 'vote': 1027, 'ted': 1028, 'cruz': 1029, 'confident': 1030, 'ceremony': 1031, 'finished': 1032, 'they': 1033, 'allow': 1034, 'mid': 1035, 'licenses': 1036, 'copyrighted': 1037, 'music': 1038, 'streaming': 1039, 'presidency': 1040, 'presently': 1041, 'christian': 1042, 'married': 1043, 'man': 1044, 'ready': 1045, 'either': 1046, 'both': 1047, 'same': 1048, 'price': 1049, 'fell': 1050, 'safe': 1051, 'tour': 1052, 'average': 1053, 'pakistan': 1054, 'complete': 1055, 'beginner': 1056, 'www': 1057, '.laracast': 1058, '.com': 1059, 'tool': 1060, 'important': 1061, 'prime': 1062, 'canvas': 1063, 'painting': 1064, 'acrylics': 1065, 'ultimate': 1066, 'retail': 1067, 'mbbs': 1068, 'corps': 1069, 'tough': 1070, 'procedure': 1071, 'questions': 1072, 'feed': 1073, 'people': 1074, 'usmle': 1075, 'hungry': 1076, 'running': 1077, 'blurb': 1078, 'n': 1079, 'participate': 1080, 'select': 1081, 'ugly': 1082, 'inch': 1083, 'dick': 1084, 'pleasure': 1085, 'request': 1086, 'std': 1087, 'astronomy': 1088, 'nasa': 1089, 'collages': 1090, 'frustrated': 1091, 'orphan': 1092, 'homeless': 1093, 'looked': 1094, 'erised': 1095, 'piano': 1096, 'skinny': 1097, 'develop': 1098, 'fit': 1099, 'gymnastics': 1100, 'free': 1101, 'hashtag': 1102, 'percentage': 1103, 'rts': 1104, 'original': 1105, 'tweets': 1106, 'replies': 1107, 'specific': 1108, 'gap': 1109, '.e': 1110, 'officer': 1111, 'down': 1112, 'moon': 1113, 'along': 1114, 'book': 1115, 'walmart': 1116, 'employee': 1117, 'discount': 1118, 'biology': 1119, 'likes': 1120, 'daily': 1121, 'multivitamins': 1122, 'bse': 1123, 'point': 1124, 'investor': 1125, 'entitled': 1126, 'overtime': 1127, 'pay': 1128, 'casino': 1129, 'dice': 1130, 'roll': 1131, 'win': 1132, 'paid': 1133, 'until': 1134, 'then': 1135, 'expected': 1136, 'payout': 1137, 'confused': 1138, 'insecure': 1139, 'short': 1140, 'lesbian': 1141, 'practical': 1142, 'driving': 1143, 'expert': 1144, 'sinful': 1145, 'females': 1146, 'wear': 1147, 'trainers': 1148, 'sneakers': 1149, 'almost': 1150, 'knows': 1151, 'naturally': 1152, 'scratch': 1153, 'yoga': 1154, 'believe': 1155, 'engineers': 1156, 'fight': 1157, 'climate': 1158, 'dellhi': 1159, 'vocal': 1160, 'range': 1161, 'd': 1162, 'f': 1163, 'type': 1164, 'voice': 1165, 'angry': 1166, 'won': 1167, 'aspects': 1168, 'him': 1169, 'winning': 1170, 'developer': 1171, 'contribute': 1172, 'open': 1173, 'source': 1174, 'github': 1175, 'big': 1176, 'fan': 1177, 'naruto': 1178, 'shippuden': 1179, 'subtitles': 1180, 'episodes': 1181, 'downloaded': 1182, 'kissanime': 1183, 'sharjah': 1184, 'uae': 1185, 'hairs': 1186, 'started': 1187, 'prevent': 1188, 'loss': 1189, 'flexible': 1190, 'teen': 1191, 'did': 1192, 'land': 1193, 'october': 1194, 'btw': 1195, 'intrested': 1196, 'exploring': 1197, 'nite': 1198, '.pls': 1199, '.thanks': 1200, 'intelligent': 1201, 'automobile': 1202, 'designer': 1203, 'civil': 1204, 'aspiring': 1205, 'subjects': 1206, 'opt': 1207, 'mains': 1208, 'pursuing': 1209, 'nsit': 1210, 'mit': 1211, 'stanford': 1212, 'american': 1213, 'sophomore': 1214, 'abroad': 1215, 'higher': 1216, 'paper': 1217, 'taller': 1218, 'spots': 1219, 'different': 1220, 'fiancee': 1221, 'emotional': 1222, 'last': 1223, 'ca': 1224, 'finals': 1225, 'sixth': 1226, 'made': 1227, 'data': 1228, 'scientist': 1229, 'prefer': 1230, 'zs': 1231, 'associates': 1232, 'technology': 1233, 'analyst': 1234, 'imbalance': 1235, 'ssc': 1236, 'cgl': 1237, 'switch': 1238, 'private': 1239, 'firm': 1240, 'typically': 1241, 'regular': 1242, 'taken': 1243, 'tests': 1244, 'contributing': 1245, 'communities': 1246, 'familiar': 1247, 'enjoy': 1248, 'beer': 1249, 'spite': 1250, 'its': 1251, 'bitter': 1252, 'taste': 1253, 'british': 1254, 'keep': 1255, 'non': 1256, 'eu': 1257, 'photographer': 1258, 'probable': 1259, 'black': 1260, 'racist': 1261, 'refer': 1262, 'falls': 1263, 'domain': 1264, 'reduce': 1265, 'usa': 1266, 'kvpy': 1267, 'sa': 1268, 'able': 1269, 'skip': 1270, 'jesus': 1271, 'consistently': 1272, 'nearly': 1273, 'yet': 1274, 'whether': 1275, 'done': 1276, 'extra': 1277, 'discontinue': 1278, 'policies': 1279, 'singapore': 1280, 'couple': 1281, 'dos': 1282, 'placed': 1283, 'increasing': 1284, 'lifting': 1285, 'crack': 1286, 'extreme': 1287, 'liberal': 1288, 'conservative': 1289, 'paths': 1290, 'intj': 1291, 'pretty': 1292, 'sure': 1293, 'mind': 1294, 'wanders': 1295, 'productive': 1296, 'off': 1297, 'sx': 1298, 'subject': 1299, 'maths': 1300, 'im': 1301, 'mixed': 1302, 'white': 1303, 'search': 1304, 'cell': 1305, 'drugs': 1306, 'developed': 1307, '.should': 1308, 'wait': 1309, 'confess': 1310, 'scrapped': 1311, 'sixties': 1312, 'happen': 1313, 'becomes': 1314, 'vlsi': 1315, 'jobs': 1316, 'london': 1317, 'wednesday': 1318, 'thing': 1319, 'staying': 1320, 'six': 1321, 'newzealand': 1322, 'broker': 1323, 'healthy': 1324, 'foods': 1325, 'maintain': 1326, 'youthful': 1327, 'rejecting': 1328, 'media': 1329, 'line': 1330, 'attract': 1331, 'past': 1332, 'bhel': 1333, 'evidence': 1334, 'coward': 1335, 'brave': 1336, '.but': 1337, 'introvert': 1338, 'cricket': 1339, 'graduate': 1340, 'got': 1341, 'tcs': 1342, 'felt': 1343, 'guilty': 1344, 'missing': 1345, 'undergraduate': 1346, 'commit': 1347, 'suicide': 1348, 'says': 1349, 'loves': 1350, 'wrong': 1351, 'bullied': 1352, 'jobless': 1353, 'anonymous': 1354, 'delicious': 1355, 'chocolate': 1356, 'cake': 1357, 'internships': 1358, 'government': 1359, 'stopped': 1360, 'strategy': 1361, 'writing': 1362, 'bba': 1363, 'ug': 1364, 'coaching': 1365, 'vmc': 1366, 'weak': 1367, 'wanted': 1368, 'extremely': 1369, 'without': 1370, 'tools': 1371, 'resources': 1372, 'speaking': 1373, 'loved': 1374, 'didn': 1375, 'drive': 1376, 'driver': 1377, 'license': 1378, 'mean': 1379, 'sleep': 1380, 'choose': 1381, 'cut': 1382, 'chubby': 1383, 'means': 1384, 'freelancer': 1385, 'hsc': 1386, 'board': 1387, 'eligible': 1388, 'advanced': 1389, 'department': 1390, 'highly': 1391, 'robotics': 1392, 'utilize': 1393, 'salary': 1394, 'savings': 1395, 'strengthen': 1396, 'structures': 1397, 'algorithms': 1398, 'update': 1399, 'ios': 1400, 'expensive': 1401, 'quick': 1402, 'wanting': 1403, 'water': 1404, 'drink': 1405, 'glowing': 1406, 'skin': 1407, 'romanian': 1408, 'chinese': 1409, 'emigrate': 1410, 'employment': 1411, 'searching': 1412, 'societal': 1413, 'collapse': 1414, 'richard': 1415, 'dawkins': 1416, 'creative': 1417, 'writer': 1418, 'recently': 1419, 'blog': 1420, 'traffic': 1421, 'someday': 1422, 'laser': 1423, 'removal': 1424, 'pass': 1425, 'manufacturing': 1426, 'interest': 1427, 'profile': 1428, 'mba': 1429, 'finance': 1430, 'schools': 1431, 'investment': 1432, 'analysis': 1433, 'portfolio': 1434, 'placements': 1435, 'blogger': 1436, 'guest': 1437, 'blogs': 1438, 'procrastinate': 1439, 'learnt': 1440, 'c': 1441, 'handling': 1442, 'files': 1443, 'enhance': 1444, 'games': 1445, 'afraid': 1446, 'death': 1447, 'fear': 1448, 'command': 1449, 'damage': 1450, 'smoking': 1451, 'suzuki': 1452, 'gixxer': 1453, 'honda': 1454, 'hornet': 1455, 'r': 1456, 'seneca': 1457, 'markham': 1458, 'housing': 1459, 'costs': 1460, 'close': 1461, 'location': 1462, 'cinque': 1463, 'terre': 1464, 'portofino': 1465, 'french': 1466, 'teaching': 1467, 'brand': 1468, 'logo': 1469, 'branding': 1470, 'boards': 1471, 'tv': 1472, 'show': 1473, 'travel': 1474, 'kolkata': 1475, 'via': 1476, 'kuala': 1477, 'lumpur': 1478, 'malaysia': 1479, 'air': 1480, 'asia': 1481, 'transit': 1482, 'passport': 1483, 'expires': 1484, 'issue': 1485, 'hot': 1486, 'teenager': 1487, 'csat': 1488, 'distant': 1489, 'cousin': 1490, 'our': 1491, 'html': 1492, 'css': 1493, 'js': 1494, 'younger': 1495, 'simultaneously': 1496, 'cdse': 1497, 'afcat': 1498, 'basic': 1499, 'eager': 1500, 'lines': 1501, 'palm': 1502, '.if': 1503, 'photos': 1504, 'tells': 1505}), ('word2count', {'i': 840, 'am': 216, 'my': 196, 'weight': 77, 'is': 158, 'kg': 9, 'and': 191, 'height': 30, '.': 301, 'ft': 6, 'what': 271, 'should': 167, 'do': 248, 'to': 350, 'gain': 9, '?': 696, 'can': 143, 'when': 11, 'm': 122, 'how': 235, 'trump': 11, 'becoming': 4, 'the': 182, 'president': 10, 'affect': 5, 'indians': 1, 'applying': 3, 'for': 134, 'an': 51, 'ms': 5, 'in': 185, 'us': 15, 'mech': 1, 'kindly': 1, 'tell': 4, 'me': 48, 'whole': 4, 'process': 1, 'of': 102, 'admission': 2, 'at': 34, 'vits': 1, 'vellore': 1, 'biotech': 1, '.i': 3, 'a': 257, 'bio': 1, 'student': 33, 'don': 9, 't': 13, 'have': 45, 'math': 3, 'there': 13, 'improve': 15, 'english': 10, 'grammar': 6, 'easiest': 6, 'way': 24, 'earn': 11, 'money': 48, 'from': 37, 'online': 39, 'good': 24, 'shape': 1, 'but': 23, 'trouble': 1, 'spot': 1, 'cellulite': 2, 'right': 5, 'under': 4, 'glutes': 1, 'top': 7, 'thighs': 2, 'get': 49, 'rid': 3, 'this': 26, 'petite': 1, 'year': 69, 'old': 72, 'woman': 5, 'who': 11, 'looks': 1, 'very': 15, 'young': 5, 'despite': 2, 'years': 34, 'work': 7, 'experience': 3, 'it': 69, 's': 23, 'disadvantage': 1, 'profession': 1, 'are': 79, 'some': 23, 'ways': 19, 'look': 2, 'act': 1, 'older': 2, 'more': 10, 'authoritative': 1, 'be': 47, 'increased': 1, 'after': 25, 'or': 35, 'age': 10, 'too': 40, 'heavy': 3, 'as': 32, 'pair': 2, 'skater': 1, 'lb': 1, 'late': 30, '!': 1, 'which': 46, 'best': 65, 'institute': 1, 'lern': 1, 'fashion': 1, 'designing': 1, 'pune': 2, 'will': 28, 'give': 9, 'scope': 1, 'skill': 3, 'while': 2, 'woking': 1, 'love': 19, 'with': 44, 'classmate': 1, 'gay': 11, 'he': 7, 'straight': 8, 'if': 45, 'muslim': 3, 'country': 2, 'suck': 4, 'up': 9, 'lose': 58, 'could': 7, 'control': 6, 'emotions': 2, 'negative': 4, 'thoughts': 1, 'entering': 2, 'world': 1, 'video': 3, 'game': 5, 'programming': 15, 'want': 62, 'know': 19, 'language': 6, 'learn': 35, 'because': 6, 'so': 15, 'many': 6, 'languages': 6, 'not': 20, 'one': 20, 'start': 53, 'you': 65, 'recommend': 7, 'that': 34, 'easy': 7, 'used': 1, 'platforms': 1, 'use': 5, 'javascript': 1, 'competitive': 1, 'stock': 3, 'market': 3, 'teach': 2, 'myself': 9, 'basics': 2, 'build': 3, 'self': 11, 'esteem': 6, 'advice': 9, 'first': 12, 'time': 10, 'entrepreneur': 9, 'founder': 1, 'ceo': 2, 'on': 30, 'hires': 1, 'still': 12, 'college': 13, 'information': 3, 'security': 2, 'order': 2, 'those': 1, 'large': 1, 'firms': 1, 'like': 11, 'google': 2, 'oracle': 1, 'facebook': 1, 'need': 8, 'manage': 1, 'marks': 3, 'all': 5, 'sections': 1, 'cat': 2, 'taking': 4, 'into': 11, 'consideration': 1, 'much': 9, 'percentile': 2, 'now': 13, 'donald': 4, 'international': 7, 'students': 10, 'stop': 12, 'coming': 1, 'universities': 3, 'become': 34, 'successful': 2, 'politician': 1, 'girl': 19, 'overweight': 4, 'feet': 5, 'inches': 4, 'tall': 7, 'weigh': 3, 'pounds': 3, 'why': 13, 'even': 2, 'fair': 1, 'skinned': 1, 'women': 2, 'crotch': 2, 'inner': 1, 'near': 3, 'dark': 1, 'colour': 1, 'concentrate': 3, 'studies': 3, 'instead': 1, 'thinking': 2, 'about': 19, 'others': 1, 'shy': 6, 'unable': 5, 'talk': 4, 'any': 16, 'nervous': 4, 'freaked': 3, 'out': 19, 'around': 4, 'them': 5, 'solution': 3, 'buy': 12, 'redmi': 2, 'note': 1, 'come': 7, 'family': 7, 'friends': 10, 'friend': 9, 'already': 3, 'boyfriend': 6, 'she': 12, 'doesn': 1, 'going': 19, 'approach': 1, 'dream': 4, 'would': 21, 'increase': 13, 'his': 2, 'gym': 1, 'since': 1, 'months': 4, 'proteins': 1, 'cleaning': 1, 'face': 10, 'regularly': 1, 'having': 4, 'pustules': 1, 'cheeks': 1, 'homosexuals': 1, 'attracted': 3, 'themselves': 2, 'by': 11, 'looking': 6, 'mirror': 2, 'were': 3, 'united': 3, 'states': 3, 'places': 8, 'goa': 12, 'visit': 11, 'nowhere': 1, 're': 4, 'given': 1, 'million': 1, 'dollars': 1, 'post': 4, 'marriage': 4, 'fallen': 1, 'incredible': 1, 'whom': 1, 'known': 1, 'than': 4, 'problem': 3, 'has': 8, 'starting': 8, 'new': 13, 'job': 10, 'next': 4, 'week': 5, 'just': 7, 'received': 1, 'call': 1, 'interview': 2, 'recruiter': 1, 'situation': 1, 'b': 17, '.tech': 12, 'ece': 4, 'fresher': 2, 'electronics': 7, 'companies': 6, 'jaipur': 1, 'apply': 2, 'feel': 9, 'tired': 2, 'life': 20, 'invest': 7, 'lack': 6, 'knowledge': 3, 'share': 1, 'proceed': 1, 'guys': 2, 'depressed': 5, 'study': 10, 'medicine': 4, 'working': 8, 'quality': 3, 'smart': 4, 'textiles': 3, 'method': 3, 'play': 8, 'guitar': 3, 'confidence': 9, 'company': 11, 'provides': 1, 'seo': 10, 'services': 4, 'delhi': 12, 'alone': 4, 'things': 5, 'advise': 1, 'turn': 6, 'turning': 5, 'month': 6, 'india': 16, 'python': 8, 'may': 2, 'body': 4, 'platform': 1, 'chemical': 1, 'engineer': 10, 'offer': 1, 'canada': 3, 'join': 7, 'indian': 10, 'army': 2, 'days': 11, 'period': 6, 'pregnant': 7, 'code': 1, 'go': 21, 'worried': 4, 'future': 2, 'jumping': 1, 'skipping': 2, 'male': 7, 'iit': 4, 'jee': 4, 'aspirant': 2, 'concepts': 2, 'clear': 1, 'full': 2, 'physics': 2, 'organic': 1, 'well': 6, 'three': 2, 'digit': 2, 'rank': 2, 'laptop': 4, 'intel': 1, 'core': 4, 'quad': 1, 'processor': 2, 'gb': 8, 'ram': 2, 'tb': 2, 'hdd': 1, 'graphics': 1, 'memory': 2, 'hours': 4, 'battery': 1, 'backup': 1, 'course': 5, 'mechanical': 15, 'engineering': 42, 'suits': 2, 'we': 8, 'make': 38, 'being': 4, 'such': 1, 'pushover': 2, 'realistically': 1, 'efficiently': 4, 'possible': 6, 'run': 2, 'seconds': 1, 'cm': 2, 'yrs': 2, 'times': 1, 'jerk': 1, 'planning': 14, 'e': 1, 'commerce': 5, 'business': 11, 'register': 1, 'easily': 6, 'rd': 3, 'published': 2, 'journal': 2, 'ijera': 1, 'mini': 1, 'project': 4, 'research': 2, 'application': 1, 'cgpa': 1, 'th': 7, 'semester': 2, 'iisc': 1, 'few': 1, 'was': 4, 'let': 4, 'regrets': 2, 'slowly': 1, 'slipping': 1, 'depression': 7, 'deal': 1, 'weeks': 2, 'castor': 1, 'oil': 1, 'early': 1, 'delievery': 1, 'preparation': 16, 'ias': 16, 'exam': 13, 'waterproof': 1, 'dslr': 1, 'camera': 1, 'uk': 5, 'tier': 2, 'general': 1, 'visa': 6, 'mma': 1, 'fighter': 1, 'training': 2, 'skilled': 2, 'enough': 4, 'android': 3, 'app': 2, 'development': 6, 'machine': 3, 'learning': 10, 'where': 11, 'web': 2, 'clients': 2, 'hedge': 2, 'fund': 3, 'using': 4, 'internet': 2, 'quiet': 1, 'always': 4, 'been': 1, 'mostly': 1, 'something': 3, 'innovative': 5, 'zero': 1, 'level': 2, 'really': 10, 'java': 1, 'programmer': 1, 'goal': 2, 'cyber': 1, 'birthday': 1, 'today': 4, 'completing': 6, 'nd': 2, 'computer': 4, 'gate': 2, 'stream': 6, 'cs': 2, 'youtube': 6, 'gaming': 4, 'channel': 6, 'words': 3, 'encouragement': 3, 'helpful': 3, 'tips': 5, 'includingexperience': 1, 'nepalese': 1, '.sc': 1, 'calcutta': 1, 'university': 4, 'phd': 2, 'astrophysics': 1, 'iia': 1, 'banglore': 1, 'ncra': 1, 'tifr': 1, 'millionaire': 7, 'before': 6, 'bangalore': 2, 'day': 5, 'great': 2, 'name': 3, 'quora': 6, 'answer': 4, 'question': 2, 'anonymously': 1, 'celebrate': 1, 'diwali': 1, 'festival': 1, 'lights': 1, 'loose': 3, 'final': 13, 'electrical': 21, 'projects': 6, 'prepare': 6, 'campus': 2, 'placement': 1, 'soon': 5, 'home': 3, 'eligibility': 1, 'criteria': 1, 'doing': 11, 'germany': 1, 'professional': 4, 'gambling': 1, 'career': 22, 'reach': 1, 'billionaire': 3, 'status': 1, 'your': 9, 'feeling': 5, 'low': 3, 'cool': 3, 'school': 21, 'bag': 1, 'carry': 1, 'during': 1, 'think': 6, 'growth': 3, 'trip': 9, 'january': 1, 'stay': 2, 'limited': 1, 'budget': 2, 'rs': 2, 'grow': 3, 'no': 4, 'idea': 5, 'figure': 3, 'female': 7, 'neanderthal': 3, 'men': 2, 'suicidal': 1, 'footballer': 1, 'actually': 4, 'bad': 6, 'fighting': 1, 'someone': 3, 'employed': 1, 'parents': 3, 'directors': 1, 'venture': 1, 'their': 4, 'recommendation': 1, 'letters': 2, 'second': 1, 'graduation': 6, 'city': 2, 'return': 1, 'hometown': 1, 'reactions': 1, 'nervousness': 1, 'considering': 3, 'dumb': 1, 'shall': 2, 'lot': 11, 'mpstme': 1, 'nmims': 1, 'enrolled': 1, 'btech': 2, 'mumbai': 2, 'affiliated': 1, 'master': 1, 'eat': 2, 'k': 3, 'race': 2, 'appearing': 1, 'executive': 1, 'december': 1, 'books': 4, 'reading': 2, 'significantly': 1, 'medical': 15, 'cab': 1, 'uber': 1, 'ola': 1, 'profitable': 1, 'car': 1, 'purchase': 1, 'less': 1, 'feminine': 1, 'mutual': 3, 'funds': 2, 'plan': 17, 'here': 1, 'onwards': 1, 'find': 15, 'housewife': 1, 'resigned': 1, 'software': 6, 'opportunities': 1, 'option': 3, 'instrumentation': 1, 'sector': 3, 'graphic': 2, 'card': 3, 'hard': 3, 'disk': 1, '.which': 1, 'trying': 3, 'larger': 3, 'butt': 3, 'bodyweight': 3, 'exercises': 3, 'reps': 3, 'sets': 3, 'experienced': 1, 'field': 5, 'design': 5, 'courses': 2, 'help': 7, 'little': 3, 'preparing': 10, 'positive': 2, 'does': 5, 'take': 5, 'cargo': 1, 'plane': 1, 'pilot': 4, 'quickly': 1, 'studying': 9, 'class': 9, 'pcm': 3, 'entrance': 1, 'exams': 2, 'other': 6, 'options': 14, 'excluding': 1, 'joining': 1, 'mncs': 1, 'own': 6, 'explain': 2, 'concept': 1, 'model': 1, 'prospects': 1, 'better': 4, 'meet': 2, 'guy': 8, 'tummy': 1, 'fat': 6, 'idhtt': 1, 'slang': 1, '': 3, 'secured': 1, 'acrobat': 1, 'introduce': 1, 'olds': 1, 'child': 1, 'had': 5, 'supposed': 1, 'commercial': 1, 'living': 1, 'ideal': 2, 'diet': 11, 'follow': 2, 'overcome': 6, 'wedding': 1, 'never': 4, 'girlfriend': 3, 'martial': 1, 'arts': 1, 'anyone': 3, 'suggest': 4, 'must': 4, 'social': 4, 'junior': 1, 'high': 4, 'graduating': 2, 'particularly': 1, 'circuit': 1, 'area': 2, 'lazy': 1, 'sometimes': 1, 'change': 7, 'standard': 1, 'spend': 2, 'iphone': 7, 'rap': 1, 'artist': 1, 'inspiration': 2, 'write': 5, 'songs': 2, 'anymore': 1, 'boost': 1, 'detail': 2, 'developing': 2, 'websites': 1, 'presentation': 1, 'among': 1, 'mischievous': 1, 'group': 1, 'often': 1, 'finds': 1, 'fun': 1, 'everything': 1, 'lowers': 1, '.what': 3, 'mres': 1, 'biomedical': 1, 'science': 4, 'topic': 1, 'poster': 1, 'suggestions': 3, 'royal': 2, 'enfield': 2, 'thunderbird': 1, 'cc': 1, 'long': 4, 'tours': 1, 'similiar': 1, 'bikes': 1, 'realistic': 4, 'formula': 1, 'live': 1, 'fall': 4, 'gets': 1, 'elected': 3, 'download': 1, 'mobile': 1, 'phone': 1, 'interested': 3, 'construction': 2, 'ask': 3, 'technical': 1, 'doubts': 1, 'related': 1, 'begin': 3, 'delve': 1, 'consultancy': 1, 'bsc': 1, 'biz': 1, 'mgt': 2, 'continue': 2, 'msc': 1, 'fields': 2, 'md': 1, 'internship': 2, 'switzerland': 1, 'ibm': 1, 'held': 1, 'took': 1, 'test': 2, 'morning': 1, 'came': 1, 'back': 2, 've': 4, 'pregnancy': 2, 'symptoms': 1, 'head': 1, 'suffering': 1, 'exercise': 3, 'eating': 4, 'drinking': 4, 'significant': 2, 'amount': 4, 'currently': 6, 'various': 2, 'materials': 1, 'proper': 1, 'stuff': 1, 'miss': 1, 'micro': 1, 'sim': 2, 'adapter': 1, 'put': 2, 'nano': 1, 'samsung': 1, 'galaxy': 1, 'provider': 1, 'most': 2, 'essential': 2, 'report': 1, 'fault': 1, 'talktalk': 1, 'scored': 1, 'cbse': 2, 'term': 3, 'chance': 1, 'performing': 1, 'scoring': 1, 'joined': 1, 'fiitjee': 1, 'please': 2, 'reasons': 1, 'management': 2, 'anything': 1, 'exercising': 1, 'every': 3, 'seven': 1, 'loneliness': 1, 'unpredictably': 1, 'moody': 1, 'fix': 2, 'convince': 2, 'mom': 1, 'sign': 1, 'local': 1, 'drivers': 1, 'education': 2, 'total': 1, 'failure': 2, 'wasted': 1, 'absolutely': 1, 'nothing': 2, 'laziness': 1, '.can': 1, 'track': 1, 'doctor': 3, 'tata': 1, 'safari': 1, 'storme': 1, 'mahindra': 1, 'xuv': 1, 'goals': 1, 'colleges': 1, 'try': 2, 'citizen': 2, 'niece': 1, 'her': 9, 'green': 1, 'faster': 1, 'gsoc': 2, 'getting': 2, 'daughter': 3, 'suited': 1, 'iot': 1, 'nit': 1, 'through': 2, 'multi': 1, 'sad': 2, 'lower': 1, 'below': 2, 'transgender': 1, 'simulate': 1, 'kerala': 1, 'prove': 1, 'only': 4, 'person': 3, 'matrix': 1, 'everyone': 1, 'else': 1, 'program': 1, 'lic': 1, 'jeevan': 4, 'anand': 2, 'saral': 2, 'including': 2, 'experiences': 2, 'attempt': 1, 'cpt': 1, 'june': 2, 'syllabus': 1, 'applicable': 1, 'available': 3, 'pursue': 4, '.designing': 1, 'st': 4, 'part': 2, 'robocon': 3, 'team': 2, 'told': 1, 'crush': 2, 'wants': 6, 'these': 1, 'feelings': 3, 'legal': 1, 'date': 2, 'completion': 1, '.pharm': 1, 'letter': 2, 'word': 1, 'normally': 1, 'u': 8, 'remove': 2, 'll': 2, 'above': 1, 'cant': 1, 'see': 4, 'interesting': 1, 'solves': 1, 'limit': 1, 'also': 5, 'send': 2, 'grps': 1, 'enter': 3, 'countries': 1, 'valid': 2, 'decision': 2, 'making': 5, 'windows': 1, 'pro': 1, 'touchpad': 1, 'scrolled': 1, 'two': 5, 'finger': 1, '.how': 1, 'fixed': 1, 'senior': 2, 'dropout': 1, 'armed': 1, 'forces': 1, 'third': 1, 'upsc': 3, 'ibps': 1, 'bank': 1, 'po': 1, '.am': 1, 'pregnent': 1, 'hindu': 2, 'boy': 5, 'marry': 5, 'mother': 1, 'brother': 1, 'cse': 1, 'product': 1, 'based': 1, 'slim': 3, 'talking': 1, 'asking': 1, 'girls': 4, 'over': 5, 'digital': 2, 'marketing': 3, 'animes': 1, 'activities': 1, 'adults': 1, 'burnt': 1, 'fastest': 3, 'tricks': 1, 'introverts': 1, 'care': 1, 'aleppo': 1, 'coding': 2, 'skills': 4, 'though': 1, 'completed': 1, 'degree': 1, 'permanently': 1, 'viewing': 1, 'watching': 1, 'pornography': 1, 'masturbatory': 1, 'aid': 1, 'lost': 1, 'greater': 1, 'god': 2, 'evil': 1, 'devil': 1, 'poor': 1, 'rich': 1, 'die': 1, 'riddle': 1, 'server': 2, 'side': 2, 'affordable': 1, 'accommodation': 1, 'rental': 2, 'service': 4, 'zealand': 1, 'earning': 3, 'photography': 1, 'tutorials': 1, 'single': 1, 'prone': 1, 'accidents': 1, 'minimum': 2, 'insurance': 1, 'required': 2, 'state': 1, 'absolute': 1, 'rate': 1, 'website': 5, 'bike': 2, 'lakhs': 1, 'south': 1, 'africa': 1, 'ncr': 2, 'optimize': 1, 'underemployed': 1, 'underpaid': 1, 'facial': 2, 'hair': 5, 'chin': 1, 'mustache': 1, 'beard': 1, 'vote': 2, 'ted': 1, 'cruz': 1, 'confident': 2, 'ceremony': 1, 'finished': 1, 'they': 3, 'allow': 1, 'mid': 1, 'licenses': 1, 'copyrighted': 1, 'music': 1, 'streaming': 1, 'presidency': 2, 'presently': 1, 'christian': 2, 'married': 1, 'man': 3, 'ready': 1, 'either': 1, 'both': 3, 'same': 2, 'price': 1, 'fell': 1, 'safe': 1, 'tour': 1, 'average': 1, 'pakistan': 1, 'complete': 1, 'beginner': 2, 'www': 1, '.laracast': 1, '.com': 1, 'tool': 3, 'important': 2, 'prime': 1, 'canvas': 1, 'painting': 1, 'acrylics': 1, 'ultimate': 1, 'retail': 1, 'mbbs': 1, 'corps': 1, 'tough': 1, 'procedure': 1, 'questions': 1, 'feed': 1, 'people': 3, 'usmle': 1, 'hungry': 1, 'running': 2, 'blurb': 1, 'n': 1, 'participate': 1, 'select': 2, 'ugly': 3, 'inch': 1, 'dick': 1, 'pleasure': 1, 'request': 1, 'std': 1, 'astronomy': 1, 'nasa': 1, 'collages': 1, 'frustrated': 3, 'orphan': 1, 'homeless': 1, 'looked': 1, 'erised': 1, 'piano': 6, 'skinny': 1, 'develop': 1, 'fit': 1, 'gymnastics': 1, 'free': 2, 'hashtag': 2, 'percentage': 1, 'rts': 1, 'original': 1, 'tweets': 1, 'replies': 1, 'specific': 1, 'gap': 1, '.e': 2, 'officer': 1, 'down': 1, 'moon': 3, 'along': 1, 'book': 3, 'walmart': 1, 'employee': 1, 'discount': 1, 'biology': 6, 'likes': 2, 'daily': 1, 'multivitamins': 1, 'bse': 1, 'point': 1, 'investor': 1, 'entitled': 1, 'overtime': 1, 'pay': 1, 'casino': 1, 'dice': 1, 'roll': 1, 'win': 2, 'paid': 1, 'until': 2, 'then': 3, 'expected': 1, 'payout': 1, 'confused': 2, 'insecure': 1, 'short': 1, 'lesbian': 4, 'practical': 2, 'driving': 2, 'expert': 1, 'sinful': 1, 'females': 1, 'wear': 1, 'trainers': 1, 'sneakers': 1, 'almost': 1, 'knows': 1, 'naturally': 1, 'scratch': 1, 'yoga': 2, 'believe': 2, 'engineers': 1, 'fight': 1, 'climate': 1, 'dellhi': 1, 'vocal': 1, 'range': 1, 'd': 1, 'f': 2, 'type': 1, 'voice': 1, 'angry': 1, 'won': 1, 'aspects': 1, 'him': 1, 'winning': 1, 'developer': 2, 'contribute': 1, 'open': 2, 'source': 3, 'github': 2, 'big': 1, 'fan': 1, 'naruto': 1, 'shippuden': 2, 'subtitles': 1, 'episodes': 1, 'downloaded': 1, 'kissanime': 1, 'sharjah': 1, 'uae': 1, 'hairs': 1, 'started': 4, 'prevent': 1, 'loss': 1, 'flexible': 1, 'teen': 1, 'did': 3, 'land': 2, 'october': 1, 'btw': 1, 'intrested': 1, 'exploring': 1, 'nite': 1, '.pls': 1, '.thanks': 1, 'intelligent': 1, 'automobile': 1, 'designer': 1, 'civil': 4, 'aspiring': 1, 'subjects': 2, 'opt': 1, 'mains': 1, 'pursuing': 2, 'nsit': 1, 'mit': 1, 'stanford': 1, 'american': 1, 'sophomore': 1, 'abroad': 1, 'higher': 1, 'paper': 1, 'taller': 2, 'spots': 1, 'different': 1, 'fiancee': 1, 'emotional': 1, 'last': 2, 'ca': 2, 'finals': 1, 'sixth': 1, 'made': 1, 'data': 5, 'scientist': 2, 'prefer': 1, 'zs': 1, 'associates': 1, 'technology': 1, 'analyst': 1, 'imbalance': 1, 'ssc': 1, 'cgl': 1, 'switch': 1, 'private': 1, 'firm': 1, 'typically': 1, 'regular': 1, 'taken': 1, 'tests': 1, 'contributing': 1, 'communities': 1, 'familiar': 1, 'enjoy': 1, 'beer': 1, 'spite': 1, 'its': 1, 'bitter': 1, 'taste': 1, 'british': 1, 'keep': 1, 'non': 2, 'eu': 2, 'photographer': 1, 'probable': 1, 'black': 2, 'racist': 1, 'refer': 1, 'falls': 1, 'domain': 1, 'reduce': 2, 'usa': 1, 'kvpy': 4, 'sa': 2, 'able': 3, 'skip': 2, 'jesus': 1, 'consistently': 1, 'nearly': 1, 'yet': 1, 'whether': 2, 'done': 3, 'extra': 1, 'discontinue': 1, 'policies': 1, 'singapore': 2, 'couple': 2, 'dos': 1, 'placed': 2, 'increasing': 1, 'lifting': 1, 'crack': 1, 'extreme': 2, 'liberal': 1, 'conservative': 1, 'paths': 1, 'intj': 1, 'pretty': 2, 'sure': 1, 'mind': 1, 'wanders': 1, 'productive': 1, 'off': 2, 'sx': 1, 'subject': 3, 'maths': 1, 'im': 2, 'mixed': 1, 'white': 1, 'search': 1, 'cell': 1, 'drugs': 1, 'developed': 1, '.should': 1, 'wait': 1, 'confess': 1, 'scrapped': 1, 'sixties': 2, 'happen': 1, 'becomes': 2, 'vlsi': 1, 'jobs': 2, 'london': 1, 'wednesday': 1, 'thing': 1, 'staying': 1, 'six': 1, 'newzealand': 1, 'broker': 1, 'healthy': 1, 'foods': 1, 'maintain': 1, 'youthful': 1, 'rejecting': 1, 'media': 1, 'line': 1, 'attract': 1, 'past': 1, 'bhel': 1, 'evidence': 1, 'coward': 1, 'brave': 1, '.but': 1, 'introvert': 1, 'cricket': 1, 'graduate': 2, 'got': 1, 'tcs': 1, 'felt': 1, 'guilty': 1, 'missing': 2, 'undergraduate': 1, 'commit': 1, 'suicide': 1, 'says': 1, 'loves': 1, 'wrong': 1, 'bullied': 1, 'jobless': 1, 'anonymous': 1, 'delicious': 1, 'chocolate': 1, 'cake': 1, 'internships': 1, 'government': 1, 'stopped': 1, 'strategy': 1, 'writing': 1, 'bba': 1, 'ug': 1, 'coaching': 1, 'vmc': 1, 'weak': 1, 'wanted': 1, 'extremely': 1, 'without': 1, 'tools': 1, 'resources': 1, 'speaking': 1, 'loved': 1, 'didn': 1, 'drive': 1, 'driver': 2, 'license': 1, 'mean': 1, 'sleep': 1, 'choose': 1, 'cut': 1, 'chubby': 1, 'means': 1, 'freelancer': 1, 'hsc': 1, 'board': 1, 'eligible': 1, 'advanced': 2, 'department': 1, 'highly': 1, 'robotics': 1, 'utilize': 1, 'salary': 1, 'savings': 1, 'strengthen': 1, 'structures': 1, 'algorithms': 1, 'update': 1, 'ios': 1, 'expensive': 1, 'quick': 1, 'wanting': 1, 'water': 2, 'drink': 2, 'glowing': 1, 'skin': 1, 'romanian': 1, 'chinese': 1, 'emigrate': 1, 'employment': 1, 'searching': 1, 'societal': 1, 'collapse': 1, 'richard': 1, 'dawkins': 1, 'creative': 1, 'writer': 2, 'recently': 1, 'blog': 3, 'traffic': 3, 'someday': 1, 'laser': 1, 'removal': 1, 'pass': 1, 'manufacturing': 1, 'interest': 1, 'profile': 1, 'mba': 1, 'finance': 1, 'schools': 1, 'investment': 1, 'analysis': 1, 'portfolio': 1, 'placements': 1, 'blogger': 1, 'guest': 1, 'blogs': 1, 'procrastinate': 1, 'learnt': 1, 'c': 1, 'handling': 1, 'files': 1, 'enhance': 1, 'games': 1, 'afraid': 1, 'death': 1, 'fear': 1, 'command': 1, 'damage': 1, 'smoking': 1, 'suzuki': 1, 'gixxer': 1, 'honda': 1, 'hornet': 1, 'r': 1, 'seneca': 1, 'markham': 1, 'housing': 1, 'costs': 1, 'close': 1, 'location': 1, 'cinque': 1, 'terre': 1, 'portofino': 1, 'french': 1, 'teaching': 1, 'brand': 1, 'logo': 1, 'branding': 1, 'boards': 1, 'tv': 1, 'show': 1, 'travel': 1, 'kolkata': 1, 'via': 1, 'kuala': 2, 'lumpur': 2, 'malaysia': 1, 'air': 1, 'asia': 1, 'transit': 1, 'passport': 1, 'expires': 1, 'issue': 1, 'hot': 1, 'teenager': 1, 'csat': 1, 'distant': 1, 'cousin': 1, 'our': 1, 'html': 1, 'css': 1, 'js': 1, 'younger': 1, 'simultaneously': 1, 'cdse': 1, 'afcat': 1, 'basic': 1, 'eager': 1, 'lines': 1, 'palm': 1, '.if': 1, 'photos': 1, 'tells': 1}), ('index2word', {0: 'SOS', 1: 'EOS', 2: 'i', 3: 'am', 4: 'my', 5: 'weight', 6: 'is', 7: 'kg', 8: 'and', 9: 'height', 10: '.', 11: 'ft', 12: 'what', 13: 'should', 14: 'do', 15: 'to', 16: 'gain', 17: '?', 18: 'can', 19: 'when', 20: 'm', 21: 'how', 22: 'trump', 23: 'becoming', 24: 'the', 25: 'president', 26: 'affect', 27: 'indians', 28: 'applying', 29: 'for', 30: 'an', 31: 'ms', 32: 'in', 33: 'us', 34: 'mech', 35: 'kindly', 36: 'tell', 37: 'me', 38: 'whole', 39: 'process', 40: 'of', 41: 'admission', 42: 'at', 43: 'vits', 44: 'vellore', 45: 'biotech', 46: '.i', 47: 'a', 48: 'bio', 49: 'student', 50: 'don', 51: 't', 52: 'have', 53: 'math', 54: 'there', 55: 'improve', 56: 'english', 57: 'grammar', 58: 'easiest', 59: 'way', 60: 'earn', 61: 'money', 62: 'from', 63: 'online', 64: 'good', 65: 'shape', 66: 'but', 67: 'trouble', 68: 'spot', 69: 'cellulite', 70: 'right', 71: 'under', 72: 'glutes', 73: 'top', 74: 'thighs', 75: 'get', 76: 'rid', 77: 'this', 78: 'petite', 79: 'year', 80: 'old', 81: 'woman', 82: 'who', 83: 'looks', 84: 'very', 85: 'young', 86: 'despite', 87: 'years', 88: 'work', 89: 'experience', 90: 'it', 91: 's', 92: 'disadvantage', 93: 'profession', 94: 'are', 95: 'some', 96: 'ways', 97: 'look', 98: 'act', 99: 'older', 100: 'more', 101: 'authoritative', 102: 'be', 103: 'increased', 104: 'after', 105: 'or', 106: 'age', 107: 'too', 108: 'heavy', 109: 'as', 110: 'pair', 111: 'skater', 112: 'lb', 113: 'late', 114: '!', 115: 'which', 116: 'best', 117: 'institute', 118: 'lern', 119: 'fashion', 120: 'designing', 121: 'pune', 122: 'will', 123: 'give', 124: 'scope', 125: 'skill', 126: 'while', 127: 'woking', 128: 'love', 129: 'with', 130: 'classmate', 131: 'gay', 132: 'he', 133: 'straight', 134: 'if', 135: 'muslim', 136: 'country', 137: 'suck', 138: 'up', 139: 'lose', 140: 'could', 141: 'control', 142: 'emotions', 143: 'negative', 144: 'thoughts', 145: 'entering', 146: 'world', 147: 'video', 148: 'game', 149: 'programming', 150: 'want', 151: 'know', 152: 'language', 153: 'learn', 154: 'because', 155: 'so', 156: 'many', 157: 'languages', 158: 'not', 159: 'one', 160: 'start', 161: 'you', 162: 'recommend', 163: 'that', 164: 'easy', 165: 'used', 166: 'platforms', 167: 'use', 168: 'javascript', 169: 'competitive', 170: 'stock', 171: 'market', 172: 'teach', 173: 'myself', 174: 'basics', 175: 'build', 176: 'self', 177: 'esteem', 178: 'advice', 179: 'first', 180: 'time', 181: 'entrepreneur', 182: 'founder', 183: 'ceo', 184: 'on', 185: 'hires', 186: 'still', 187: 'college', 188: 'information', 189: 'security', 190: 'order', 191: 'those', 192: 'large', 193: 'firms', 194: 'like', 195: 'google', 196: 'oracle', 197: 'facebook', 198: 'need', 199: 'manage', 200: 'marks', 201: 'all', 202: 'sections', 203: 'cat', 204: 'taking', 205: 'into', 206: 'consideration', 207: 'much', 208: 'percentile', 209: 'now', 210: 'donald', 211: 'international', 212: 'students', 213: 'stop', 214: 'coming', 215: 'universities', 216: 'become', 217: 'successful', 218: 'politician', 219: 'girl', 220: 'overweight', 221: 'feet', 222: 'inches', 223: 'tall', 224: 'weigh', 225: 'pounds', 226: 'why', 227: 'even', 228: 'fair', 229: 'skinned', 230: 'women', 231: 'crotch', 232: 'inner', 233: 'near', 234: 'dark', 235: 'colour', 236: 'concentrate', 237: 'studies', 238: 'instead', 239: 'thinking', 240: 'about', 241: 'others', 242: 'shy', 243: 'unable', 244: 'talk', 245: 'any', 246: 'nervous', 247: 'freaked', 248: 'out', 249: 'around', 250: 'them', 251: 'solution', 252: 'buy', 253: 'redmi', 254: 'note', 255: 'come', 256: 'family', 257: 'friends', 258: 'friend', 259: 'already', 260: 'boyfriend', 261: 'she', 262: 'doesn', 263: 'going', 264: 'approach', 265: 'dream', 266: 'would', 267: 'increase', 268: 'his', 269: 'gym', 270: 'since', 271: 'months', 272: 'proteins', 273: 'cleaning', 274: 'face', 275: 'regularly', 276: 'having', 277: 'pustules', 278: 'cheeks', 279: 'homosexuals', 280: 'attracted', 281: 'themselves', 282: 'by', 283: 'looking', 284: 'mirror', 285: 'were', 286: 'united', 287: 'states', 288: 'places', 289: 'goa', 290: 'visit', 291: 'nowhere', 292: 're', 293: 'given', 294: 'million', 295: 'dollars', 296: 'post', 297: 'marriage', 298: 'fallen', 299: 'incredible', 300: 'whom', 301: 'known', 302: 'than', 303: 'problem', 304: 'has', 305: 'starting', 306: 'new', 307: 'job', 308: 'next', 309: 'week', 310: 'just', 311: 'received', 312: 'call', 313: 'interview', 314: 'recruiter', 315: 'situation', 316: 'b', 317: '.tech', 318: 'ece', 319: 'fresher', 320: 'electronics', 321: 'companies', 322: 'jaipur', 323: 'apply', 324: 'feel', 325: 'tired', 326: 'life', 327: 'invest', 328: 'lack', 329: 'knowledge', 330: 'share', 331: 'proceed', 332: 'guys', 333: 'depressed', 334: 'study', 335: 'medicine', 336: 'working', 337: 'quality', 338: 'smart', 339: 'textiles', 340: 'method', 341: 'play', 342: 'guitar', 343: 'confidence', 344: 'company', 345: 'provides', 346: 'seo', 347: 'services', 348: 'delhi', 349: 'alone', 350: 'things', 351: 'advise', 352: 'turn', 353: 'turning', 354: 'month', 355: 'india', 356: 'python', 357: 'may', 358: 'body', 359: 'platform', 360: 'chemical', 361: 'engineer', 362: 'offer', 363: 'canada', 364: 'join', 365: 'indian', 366: 'army', 367: 'days', 368: 'period', 369: 'pregnant', 370: 'code', 371: 'go', 372: 'worried', 373: 'future', 374: 'jumping', 375: 'skipping', 376: 'male', 377: 'iit', 378: 'jee', 379: 'aspirant', 380: 'concepts', 381: 'clear', 382: 'full', 383: 'physics', 384: 'organic', 385: 'well', 386: 'three', 387: 'digit', 388: 'rank', 389: 'laptop', 390: 'intel', 391: 'core', 392: 'quad', 393: 'processor', 394: 'gb', 395: 'ram', 396: 'tb', 397: 'hdd', 398: 'graphics', 399: 'memory', 400: 'hours', 401: 'battery', 402: 'backup', 403: 'course', 404: 'mechanical', 405: 'engineering', 406: 'suits', 407: 'we', 408: 'make', 409: 'being', 410: 'such', 411: 'pushover', 412: 'realistically', 413: 'efficiently', 414: 'possible', 415: 'run', 416: 'seconds', 417: 'cm', 418: 'yrs', 419: 'times', 420: 'jerk', 421: 'planning', 422: 'e', 423: 'commerce', 424: 'business', 425: 'register', 426: 'easily', 427: 'rd', 428: 'published', 429: 'journal', 430: 'ijera', 431: 'mini', 432: 'project', 433: 'research', 434: 'application', 435: 'cgpa', 436: 'th', 437: 'semester', 438: 'iisc', 439: 'few', 440: 'was', 441: 'let', 442: 'regrets', 443: 'slowly', 444: 'slipping', 445: 'depression', 446: 'deal', 447: 'weeks', 448: 'castor', 449: 'oil', 450: 'early', 451: 'delievery', 452: 'preparation', 453: 'ias', 454: 'exam', 455: 'waterproof', 456: 'dslr', 457: 'camera', 458: 'uk', 459: 'tier', 460: 'general', 461: 'visa', 462: 'mma', 463: 'fighter', 464: 'training', 465: 'skilled', 466: 'enough', 467: 'android', 468: 'app', 469: 'development', 470: 'machine', 471: 'learning', 472: 'where', 473: 'web', 474: 'clients', 475: 'hedge', 476: 'fund', 477: 'using', 478: 'internet', 479: 'quiet', 480: 'always', 481: 'been', 482: 'mostly', 483: 'something', 484: 'innovative', 485: 'zero', 486: 'level', 487: 'really', 488: 'java', 489: 'programmer', 490: 'goal', 491: 'cyber', 492: 'birthday', 493: 'today', 494: 'completing', 495: 'nd', 496: 'computer', 497: 'gate', 498: 'stream', 499: 'cs', 500: 'youtube', 501: 'gaming', 502: 'channel', 503: 'words', 504: 'encouragement', 505: 'helpful', 506: 'tips', 507: 'includingexperience', 508: 'nepalese', 509: '.sc', 510: 'calcutta', 511: 'university', 512: 'phd', 513: 'astrophysics', 514: 'iia', 515: 'banglore', 516: 'ncra', 517: 'tifr', 518: 'millionaire', 519: 'before', 520: 'bangalore', 521: 'day', 522: 'great', 523: 'name', 524: 'quora', 525: 'answer', 526: 'question', 527: 'anonymously', 528: 'celebrate', 529: 'diwali', 530: 'festival', 531: 'lights', 532: 'loose', 533: 'final', 534: 'electrical', 535: 'projects', 536: 'prepare', 537: 'campus', 538: 'placement', 539: 'soon', 540: 'home', 541: 'eligibility', 542: 'criteria', 543: 'doing', 544: 'germany', 545: 'professional', 546: 'gambling', 547: 'career', 548: 'reach', 549: 'billionaire', 550: 'status', 551: 'your', 552: 'feeling', 553: 'low', 554: 'cool', 555: 'school', 556: 'bag', 557: 'carry', 558: 'during', 559: 'think', 560: 'growth', 561: 'trip', 562: 'january', 563: 'stay', 564: 'limited', 565: 'budget', 566: 'rs', 567: 'grow', 568: 'no', 569: 'idea', 570: 'figure', 571: 'female', 572: 'neanderthal', 573: 'men', 574: 'suicidal', 575: 'footballer', 576: 'actually', 577: 'bad', 578: 'fighting', 579: 'someone', 580: 'employed', 581: 'parents', 582: 'directors', 583: 'venture', 584: 'their', 585: 'recommendation', 586: 'letters', 587: 'second', 588: 'graduation', 589: 'city', 590: 'return', 591: 'hometown', 592: 'reactions', 593: 'nervousness', 594: 'considering', 595: 'dumb', 596: 'shall', 597: 'lot', 598: 'mpstme', 599: 'nmims', 600: 'enrolled', 601: 'btech', 602: 'mumbai', 603: 'affiliated', 604: 'master', 605: 'eat', 606: 'k', 607: 'race', 608: 'appearing', 609: 'executive', 610: 'december', 611: 'books', 612: 'reading', 613: 'significantly', 614: 'medical', 615: 'cab', 616: 'uber', 617: 'ola', 618: 'profitable', 619: 'car', 620: 'purchase', 621: 'less', 622: 'feminine', 623: 'mutual', 624: 'funds', 625: 'plan', 626: 'here', 627: 'onwards', 628: 'find', 629: 'housewife', 630: 'resigned', 631: 'software', 632: 'opportunities', 633: 'option', 634: 'instrumentation', 635: 'sector', 636: 'graphic', 637: 'card', 638: 'hard', 639: 'disk', 640: '.which', 641: 'trying', 642: 'larger', 643: 'butt', 644: 'bodyweight', 645: 'exercises', 646: 'reps', 647: 'sets', 648: 'experienced', 649: 'field', 650: 'design', 651: 'courses', 652: 'help', 653: 'little', 654: 'preparing', 655: 'positive', 656: 'does', 657: 'take', 658: 'cargo', 659: 'plane', 660: 'pilot', 661: 'quickly', 662: 'studying', 663: 'class', 664: 'pcm', 665: 'entrance', 666: 'exams', 667: 'other', 668: 'options', 669: 'excluding', 670: 'joining', 671: 'mncs', 672: 'own', 673: 'explain', 674: 'concept', 675: 'model', 676: 'prospects', 677: 'better', 678: 'meet', 679: 'guy', 680: 'tummy', 681: 'fat', 682: 'idhtt', 683: 'slang', 684: '', 685: 'secured', 686: 'acrobat', 687: 'introduce', 688: 'olds', 689: 'child', 690: 'had', 691: 'supposed', 692: 'commercial', 693: 'living', 694: 'ideal', 695: 'diet', 696: 'follow', 697: 'overcome', 698: 'wedding', 699: 'never', 700: 'girlfriend', 701: 'martial', 702: 'arts', 703: 'anyone', 704: 'suggest', 705: 'must', 706: 'social', 707: 'junior', 708: 'high', 709: 'graduating', 710: 'particularly', 711: 'circuit', 712: 'area', 713: 'lazy', 714: 'sometimes', 715: 'change', 716: 'standard', 717: 'spend', 718: 'iphone', 719: 'rap', 720: 'artist', 721: 'inspiration', 722: 'write', 723: 'songs', 724: 'anymore', 725: 'boost', 726: 'detail', 727: 'developing', 728: 'websites', 729: 'presentation', 730: 'among', 731: 'mischievous', 732: 'group', 733: 'often', 734: 'finds', 735: 'fun', 736: 'everything', 737: 'lowers', 738: '.what', 739: 'mres', 740: 'biomedical', 741: 'science', 742: 'topic', 743: 'poster', 744: 'suggestions', 745: 'royal', 746: 'enfield', 747: 'thunderbird', 748: 'cc', 749: 'long', 750: 'tours', 751: 'similiar', 752: 'bikes', 753: 'realistic', 754: 'formula', 755: 'live', 756: 'fall', 757: 'gets', 758: 'elected', 759: 'download', 760: 'mobile', 761: 'phone', 762: 'interested', 763: 'construction', 764: 'ask', 765: 'technical', 766: 'doubts', 767: 'related', 768: 'begin', 769: 'delve', 770: 'consultancy', 771: 'bsc', 772: 'biz', 773: 'mgt', 774: 'continue', 775: 'msc', 776: 'fields', 777: 'md', 778: 'internship', 779: 'switzerland', 780: 'ibm', 781: 'held', 782: 'took', 783: 'test', 784: 'morning', 785: 'came', 786: 'back', 787: 've', 788: 'pregnancy', 789: 'symptoms', 790: 'head', 791: 'suffering', 792: 'exercise', 793: 'eating', 794: 'drinking', 795: 'significant', 796: 'amount', 797: 'currently', 798: 'various', 799: 'materials', 800: 'proper', 801: 'stuff', 802: 'miss', 803: 'micro', 804: 'sim', 805: 'adapter', 806: 'put', 807: 'nano', 808: 'samsung', 809: 'galaxy', 810: 'provider', 811: 'most', 812: 'essential', 813: 'report', 814: 'fault', 815: 'talktalk', 816: 'scored', 817: 'cbse', 818: 'term', 819: 'chance', 820: 'performing', 821: 'scoring', 822: 'joined', 823: 'fiitjee', 824: 'please', 825: 'reasons', 826: 'management', 827: 'anything', 828: 'exercising', 829: 'every', 830: 'seven', 831: 'loneliness', 832: 'unpredictably', 833: 'moody', 834: 'fix', 835: 'convince', 836: 'mom', 837: 'sign', 838: 'local', 839: 'drivers', 840: 'education', 841: 'total', 842: 'failure', 843: 'wasted', 844: 'absolutely', 845: 'nothing', 846: 'laziness', 847: '.can', 848: 'track', 849: 'doctor', 850: 'tata', 851: 'safari', 852: 'storme', 853: 'mahindra', 854: 'xuv', 855: 'goals', 856: 'colleges', 857: 'try', 858: 'citizen', 859: 'niece', 860: 'her', 861: 'green', 862: 'faster', 863: 'gsoc', 864: 'getting', 865: 'daughter', 866: 'suited', 867: 'iot', 868: 'nit', 869: 'through', 870: 'multi', 871: 'sad', 872: 'lower', 873: 'below', 874: 'transgender', 875: 'simulate', 876: 'kerala', 877: 'prove', 878: 'only', 879: 'person', 880: 'matrix', 881: 'everyone', 882: 'else', 883: 'program', 884: 'lic', 885: 'jeevan', 886: 'anand', 887: 'saral', 888: 'including', 889: 'experiences', 890: 'attempt', 891: 'cpt', 892: 'june', 893: 'syllabus', 894: 'applicable', 895: 'available', 896: 'pursue', 897: '.designing', 898: 'st', 899: 'part', 900: 'robocon', 901: 'team', 902: 'told', 903: 'crush', 904: 'wants', 905: 'these', 906: 'feelings', 907: 'legal', 908: 'date', 909: 'completion', 910: '.pharm', 911: 'letter', 912: 'word', 913: 'normally', 914: 'u', 915: 'remove', 916: 'll', 917: 'above', 918: 'cant', 919: 'see', 920: 'interesting', 921: 'solves', 922: 'limit', 923: 'also', 924: 'send', 925: 'grps', 926: 'enter', 927: 'countries', 928: 'valid', 929: 'decision', 930: 'making', 931: 'windows', 932: 'pro', 933: 'touchpad', 934: 'scrolled', 935: 'two', 936: 'finger', 937: '.how', 938: 'fixed', 939: 'senior', 940: 'dropout', 941: 'armed', 942: 'forces', 943: 'third', 944: 'upsc', 945: 'ibps', 946: 'bank', 947: 'po', 948: '.am', 949: 'pregnent', 950: 'hindu', 951: 'boy', 952: 'marry', 953: 'mother', 954: 'brother', 955: 'cse', 956: 'product', 957: 'based', 958: 'slim', 959: 'talking', 960: 'asking', 961: 'girls', 962: 'over', 963: 'digital', 964: 'marketing', 965: 'animes', 966: 'activities', 967: 'adults', 968: 'burnt', 969: 'fastest', 970: 'tricks', 971: 'introverts', 972: 'care', 973: 'aleppo', 974: 'coding', 975: 'skills', 976: 'though', 977: 'completed', 978: 'degree', 979: 'permanently', 980: 'viewing', 981: 'watching', 982: 'pornography', 983: 'masturbatory', 984: 'aid', 985: 'lost', 986: 'greater', 987: 'god', 988: 'evil', 989: 'devil', 990: 'poor', 991: 'rich', 992: 'die', 993: 'riddle', 994: 'server', 995: 'side', 996: 'affordable', 997: 'accommodation', 998: 'rental', 999: 'service', 1000: 'zealand', 1001: 'earning', 1002: 'photography', 1003: 'tutorials', 1004: 'single', 1005: 'prone', 1006: 'accidents', 1007: 'minimum', 1008: 'insurance', 1009: 'required', 1010: 'state', 1011: 'absolute', 1012: 'rate', 1013: 'website', 1014: 'bike', 1015: 'lakhs', 1016: 'south', 1017: 'africa', 1018: 'ncr', 1019: 'optimize', 1020: 'underemployed', 1021: 'underpaid', 1022: 'facial', 1023: 'hair', 1024: 'chin', 1025: 'mustache', 1026: 'beard', 1027: 'vote', 1028: 'ted', 1029: 'cruz', 1030: 'confident', 1031: 'ceremony', 1032: 'finished', 1033: 'they', 1034: 'allow', 1035: 'mid', 1036: 'licenses', 1037: 'copyrighted', 1038: 'music', 1039: 'streaming', 1040: 'presidency', 1041: 'presently', 1042: 'christian', 1043: 'married', 1044: 'man', 1045: 'ready', 1046: 'either', 1047: 'both', 1048: 'same', 1049: 'price', 1050: 'fell', 1051: 'safe', 1052: 'tour', 1053: 'average', 1054: 'pakistan', 1055: 'complete', 1056: 'beginner', 1057: 'www', 1058: '.laracast', 1059: '.com', 1060: 'tool', 1061: 'important', 1062: 'prime', 1063: 'canvas', 1064: 'painting', 1065: 'acrylics', 1066: 'ultimate', 1067: 'retail', 1068: 'mbbs', 1069: 'corps', 1070: 'tough', 1071: 'procedure', 1072: 'questions', 1073: 'feed', 1074: 'people', 1075: 'usmle', 1076: 'hungry', 1077: 'running', 1078: 'blurb', 1079: 'n', 1080: 'participate', 1081: 'select', 1082: 'ugly', 1083: 'inch', 1084: 'dick', 1085: 'pleasure', 1086: 'request', 1087: 'std', 1088: 'astronomy', 1089: 'nasa', 1090: 'collages', 1091: 'frustrated', 1092: 'orphan', 1093: 'homeless', 1094: 'looked', 1095: 'erised', 1096: 'piano', 1097: 'skinny', 1098: 'develop', 1099: 'fit', 1100: 'gymnastics', 1101: 'free', 1102: 'hashtag', 1103: 'percentage', 1104: 'rts', 1105: 'original', 1106: 'tweets', 1107: 'replies', 1108: 'specific', 1109: 'gap', 1110: '.e', 1111: 'officer', 1112: 'down', 1113: 'moon', 1114: 'along', 1115: 'book', 1116: 'walmart', 1117: 'employee', 1118: 'discount', 1119: 'biology', 1120: 'likes', 1121: 'daily', 1122: 'multivitamins', 1123: 'bse', 1124: 'point', 1125: 'investor', 1126: 'entitled', 1127: 'overtime', 1128: 'pay', 1129: 'casino', 1130: 'dice', 1131: 'roll', 1132: 'win', 1133: 'paid', 1134: 'until', 1135: 'then', 1136: 'expected', 1137: 'payout', 1138: 'confused', 1139: 'insecure', 1140: 'short', 1141: 'lesbian', 1142: 'practical', 1143: 'driving', 1144: 'expert', 1145: 'sinful', 1146: 'females', 1147: 'wear', 1148: 'trainers', 1149: 'sneakers', 1150: 'almost', 1151: 'knows', 1152: 'naturally', 1153: 'scratch', 1154: 'yoga', 1155: 'believe', 1156: 'engineers', 1157: 'fight', 1158: 'climate', 1159: 'dellhi', 1160: 'vocal', 1161: 'range', 1162: 'd', 1163: 'f', 1164: 'type', 1165: 'voice', 1166: 'angry', 1167: 'won', 1168: 'aspects', 1169: 'him', 1170: 'winning', 1171: 'developer', 1172: 'contribute', 1173: 'open', 1174: 'source', 1175: 'github', 1176: 'big', 1177: 'fan', 1178: 'naruto', 1179: 'shippuden', 1180: 'subtitles', 1181: 'episodes', 1182: 'downloaded', 1183: 'kissanime', 1184: 'sharjah', 1185: 'uae', 1186: 'hairs', 1187: 'started', 1188: 'prevent', 1189: 'loss', 1190: 'flexible', 1191: 'teen', 1192: 'did', 1193: 'land', 1194: 'october', 1195: 'btw', 1196: 'intrested', 1197: 'exploring', 1198: 'nite', 1199: '.pls', 1200: '.thanks', 1201: 'intelligent', 1202: 'automobile', 1203: 'designer', 1204: 'civil', 1205: 'aspiring', 1206: 'subjects', 1207: 'opt', 1208: 'mains', 1209: 'pursuing', 1210: 'nsit', 1211: 'mit', 1212: 'stanford', 1213: 'american', 1214: 'sophomore', 1215: 'abroad', 1216: 'higher', 1217: 'paper', 1218: 'taller', 1219: 'spots', 1220: 'different', 1221: 'fiancee', 1222: 'emotional', 1223: 'last', 1224: 'ca', 1225: 'finals', 1226: 'sixth', 1227: 'made', 1228: 'data', 1229: 'scientist', 1230: 'prefer', 1231: 'zs', 1232: 'associates', 1233: 'technology', 1234: 'analyst', 1235: 'imbalance', 1236: 'ssc', 1237: 'cgl', 1238: 'switch', 1239: 'private', 1240: 'firm', 1241: 'typically', 1242: 'regular', 1243: 'taken', 1244: 'tests', 1245: 'contributing', 1246: 'communities', 1247: 'familiar', 1248: 'enjoy', 1249: 'beer', 1250: 'spite', 1251: 'its', 1252: 'bitter', 1253: 'taste', 1254: 'british', 1255: 'keep', 1256: 'non', 1257: 'eu', 1258: 'photographer', 1259: 'probable', 1260: 'black', 1261: 'racist', 1262: 'refer', 1263: 'falls', 1264: 'domain', 1265: 'reduce', 1266: 'usa', 1267: 'kvpy', 1268: 'sa', 1269: 'able', 1270: 'skip', 1271: 'jesus', 1272: 'consistently', 1273: 'nearly', 1274: 'yet', 1275: 'whether', 1276: 'done', 1277: 'extra', 1278: 'discontinue', 1279: 'policies', 1280: 'singapore', 1281: 'couple', 1282: 'dos', 1283: 'placed', 1284: 'increasing', 1285: 'lifting', 1286: 'crack', 1287: 'extreme', 1288: 'liberal', 1289: 'conservative', 1290: 'paths', 1291: 'intj', 1292: 'pretty', 1293: 'sure', 1294: 'mind', 1295: 'wanders', 1296: 'productive', 1297: 'off', 1298: 'sx', 1299: 'subject', 1300: 'maths', 1301: 'im', 1302: 'mixed', 1303: 'white', 1304: 'search', 1305: 'cell', 1306: 'drugs', 1307: 'developed', 1308: '.should', 1309: 'wait', 1310: 'confess', 1311: 'scrapped', 1312: 'sixties', 1313: 'happen', 1314: 'becomes', 1315: 'vlsi', 1316: 'jobs', 1317: 'london', 1318: 'wednesday', 1319: 'thing', 1320: 'staying', 1321: 'six', 1322: 'newzealand', 1323: 'broker', 1324: 'healthy', 1325: 'foods', 1326: 'maintain', 1327: 'youthful', 1328: 'rejecting', 1329: 'media', 1330: 'line', 1331: 'attract', 1332: 'past', 1333: 'bhel', 1334: 'evidence', 1335: 'coward', 1336: 'brave', 1337: '.but', 1338: 'introvert', 1339: 'cricket', 1340: 'graduate', 1341: 'got', 1342: 'tcs', 1343: 'felt', 1344: 'guilty', 1345: 'missing', 1346: 'undergraduate', 1347: 'commit', 1348: 'suicide', 1349: 'says', 1350: 'loves', 1351: 'wrong', 1352: 'bullied', 1353: 'jobless', 1354: 'anonymous', 1355: 'delicious', 1356: 'chocolate', 1357: 'cake', 1358: 'internships', 1359: 'government', 1360: 'stopped', 1361: 'strategy', 1362: 'writing', 1363: 'bba', 1364: 'ug', 1365: 'coaching', 1366: 'vmc', 1367: 'weak', 1368: 'wanted', 1369: 'extremely', 1370: 'without', 1371: 'tools', 1372: 'resources', 1373: 'speaking', 1374: 'loved', 1375: 'didn', 1376: 'drive', 1377: 'driver', 1378: 'license', 1379: 'mean', 1380: 'sleep', 1381: 'choose', 1382: 'cut', 1383: 'chubby', 1384: 'means', 1385: 'freelancer', 1386: 'hsc', 1387: 'board', 1388: 'eligible', 1389: 'advanced', 1390: 'department', 1391: 'highly', 1392: 'robotics', 1393: 'utilize', 1394: 'salary', 1395: 'savings', 1396: 'strengthen', 1397: 'structures', 1398: 'algorithms', 1399: 'update', 1400: 'ios', 1401: 'expensive', 1402: 'quick', 1403: 'wanting', 1404: 'water', 1405: 'drink', 1406: 'glowing', 1407: 'skin', 1408: 'romanian', 1409: 'chinese', 1410: 'emigrate', 1411: 'employment', 1412: 'searching', 1413: 'societal', 1414: 'collapse', 1415: 'richard', 1416: 'dawkins', 1417: 'creative', 1418: 'writer', 1419: 'recently', 1420: 'blog', 1421: 'traffic', 1422: 'someday', 1423: 'laser', 1424: 'removal', 1425: 'pass', 1426: 'manufacturing', 1427: 'interest', 1428: 'profile', 1429: 'mba', 1430: 'finance', 1431: 'schools', 1432: 'investment', 1433: 'analysis', 1434: 'portfolio', 1435: 'placements', 1436: 'blogger', 1437: 'guest', 1438: 'blogs', 1439: 'procrastinate', 1440: 'learnt', 1441: 'c', 1442: 'handling', 1443: 'files', 1444: 'enhance', 1445: 'games', 1446: 'afraid', 1447: 'death', 1448: 'fear', 1449: 'command', 1450: 'damage', 1451: 'smoking', 1452: 'suzuki', 1453: 'gixxer', 1454: 'honda', 1455: 'hornet', 1456: 'r', 1457: 'seneca', 1458: 'markham', 1459: 'housing', 1460: 'costs', 1461: 'close', 1462: 'location', 1463: 'cinque', 1464: 'terre', 1465: 'portofino', 1466: 'french', 1467: 'teaching', 1468: 'brand', 1469: 'logo', 1470: 'branding', 1471: 'boards', 1472: 'tv', 1473: 'show', 1474: 'travel', 1475: 'kolkata', 1476: 'via', 1477: 'kuala', 1478: 'lumpur', 1479: 'malaysia', 1480: 'air', 1481: 'asia', 1482: 'transit', 1483: 'passport', 1484: 'expires', 1485: 'issue', 1486: 'hot', 1487: 'teenager', 1488: 'csat', 1489: 'distant', 1490: 'cousin', 1491: 'our', 1492: 'html', 1493: 'css', 1494: 'js', 1495: 'younger', 1496: 'simultaneously', 1497: 'cdse', 1498: 'afcat', 1499: 'basic', 1500: 'eager', 1501: 'lines', 1502: 'palm', 1503: '.if', 1504: 'photos', 1505: 'tells'}), ('n_words', 1506)])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot9BcpB3IaZA",
        "outputId": "6a0931ca-0691-4ca3-8508-8033eac8ec77"
      },
      "source": [
        "input_size = input_lang.n_words\n",
        "hidden_size = 256\n",
        "input_size"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1506"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4R6wxC-Ipkw"
      },
      "source": [
        "embedding = nn.Embedding(input_size, hidden_size)\n",
        "gru = nn.GRU(hidden_size, hidden_size)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAMIb49kI_BL",
        "outputId": "a568e0ca-30e9-441a-edc1-15565ab945d7"
      },
      "source": [
        "sample = random.choice(pairs)\n",
        "sample"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what programming languages should i learn for video game development ?',\n",
              " 'i am a year old boy that wants to learn how to program video games . what programming languages should i learn ? how do i get started ?']"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVu_qJlQK2mE",
        "outputId": "4ab274d4-a5fc-44d4-a562-1b61250c1c79"
      },
      "source": [
        "device"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "CSc3SeyQJIAg",
        "outputId": "a8d45bca-fcb7-430d-e2c3-f9462a2bca77"
      },
      "source": [
        "embedding_input = embedding(sample[0])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-fdfff51cb5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jds83uf2JOOm"
      },
      "source": [
        "sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA6M3xhyJRJW"
      },
      "source": [
        "input_sentence = sample[0]\n",
        "output_sentence = sample[1]\n",
        "\n",
        "input_lang.word2index['money']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E57BWJHJdoi"
      },
      "source": [
        "for word in input_sentence:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcQKsPxqJhlu"
      },
      "source": [
        "for word in input_sentence.split(' '):\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNfpJbYJmKg"
      },
      "source": [
        "input_indices = [input_lang.word2index[word] for word in input_sentence.split(' ')]\n",
        "output_indices = [output_lang.word2index[word] for word in output_sentence.split(' ')]\n",
        "input_indices, output_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3QSMYwoJzr8"
      },
      "source": [
        "embedding_input = embedding(input_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzvgEf40J5a2"
      },
      "source": [
        "input_indices.append(EOS_token)\n",
        "output_indices.append(EOS_token)\n",
        "input_indices, output_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2DCu5CsKCWy"
      },
      "source": [
        "input_tensor = torch.tensor(input_indices, dtype=torch.long, device=device)\n",
        "output_tensor = torch.tensor(output_indices, dtype=torch.long, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwCukAHjKRqa"
      },
      "source": [
        "input_tensor.shape, output_tensor.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxhmqv0rKV43"
      },
      "source": [
        "embedding_input = embedding(input_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIOiEKUKLDbk"
      },
      "source": [
        "embedding = nn.Embedding(input_size, hidden_size).to(device)\n",
        "gru = nn.GRU(hidden_size, hidden_size).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0pQVrIQLNA0"
      },
      "source": [
        "embedding_input = embedding(input_tensor)\n",
        "embedding_input.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWmACcb5LS1y"
      },
      "source": [
        "input_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOOjODorLgbJ"
      },
      "source": [
        "input_tensor.shape, input_tensor.view(-1, 1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7AV8GgLp-u"
      },
      "source": [
        "print(embedding_input.shape)\n",
        "embedding_input = embedding(input_tensor.view(-1, 1))\n",
        "print(embedding_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS0iGCt2L2aa"
      },
      "source": [
        "# output, hidden = gru(embedde_input, ?)\n",
        "hidden = torch.zeros(1, 1, 256, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvJDgd6tMgtL"
      },
      "source": [
        "embedding_input = embedding(input_tensor.view(-1, 1))\n",
        "output, hidden = gru(embedding_input, hidden)\n",
        "\n",
        "output.shape, output[0, 0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZAzkzkKNo_l"
      },
      "source": [
        "encoder_outputs = torch.zeros(MAX_LENGTH, 256, device=device)\n",
        "encoder_outputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozs4sykfOS8j"
      },
      "source": [
        "input_tensor.size()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZomwGYAbO0iz"
      },
      "source": [
        "encoder_outputs = torch.zeros(MAX_LENGTH, 256, device=device)\n",
        "hidden = torch.zeros(1, 1, 256, device = device)\n",
        "\n",
        "for i in range(input_tensor.size()[0]):\n",
        "  embedding_input = embedding(input_tensor[i].view(-1, 1))\n",
        "  output, hidden = gru(embedding_input, hidden)\n",
        "  encoder_outputs[i] += output[0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szh9dWLdPBII"
      },
      "source": [
        "encoder_outputs.shape, hidden.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqgndIpsQAjz"
      },
      "source": [
        "encoder_outputs[0:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umC9gtYpQCJ3"
      },
      "source": [
        "encoder_outputs[7:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XswFsPSXQWzy"
      },
      "source": [
        "# 😁\n",
        "\n",
        "Finally our Encoder is fully ready. Now let's look at the class we wrote in the last class to see what we missed!\n",
        "\n",
        "```\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "```\n",
        "\n",
        "Cool! Next let's build out Decoder where we have attention in-built.\n",
        "\n",
        "# Decoder with Attention\n",
        "\n",
        "Here is the plan. \n",
        "\n",
        "1. First input to the decoder will be SOS_token, later inputs would be the words it predicted (unless we implement teacher forcing)\n",
        "2. decoder/GRU's hidden state will be initialized with the encoder's last hidden state\n",
        "3. we will use gru's hidden state and last prediction to generate attention weight using a FC layer. \n",
        "4. this attention weight will be used to weigh the encoder_outputs using batch matric multiplication. This will give us a NEW view on how to look at encoder_states.\n",
        "5. this attention applied encoder_states will then be concatenated with the input, and then sent a linear layer and _then_ sent to the GRU. \n",
        "6. GRU's output will be sent to a FC layer to predict one of the output_language words\n",
        "\n",
        "Let's prepare all the inputs we need to do this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gQIvExHQKN1"
      },
      "source": [
        "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "decoder_hidden = hidden\n",
        "decoded_words = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2MKIEpYQnn_"
      },
      "source": [
        "# decoder s0\n",
        "output_size = output_lang.n_words\n",
        "embedding = nn.Embedding(output_size, 256).to(device)\n",
        "embedded = embedding(decoder_input)\n",
        "embedded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D8GxzPXQygF"
      },
      "source": [
        "# 256 * 2 >> after concatenation\n",
        "attn_weight_layer = nn.Linear(256 * 2, 295).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjEjHiWmR10f"
      },
      "source": [
        "embedded.shape, decoder_hidden.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTMhiykNR5H6",
        "outputId": "a64176b8-f34c-4efd-81f6-b8732994e606"
      },
      "source": [
        "torch.cat((embedded[0], decoder_hidden[0]), 1).shape"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_-gpdX6SHjd",
        "outputId": "faee9368-4c98-4712-a135-e7ba598e1171"
      },
      "source": [
        "attn_weight_layer = nn.Linear(256 * 2, 295).to(device)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0696,  0.5946, -0.2599,  0.0091, -0.2947, -0.3255, -0.3279, -0.0034,\n",
              "          0.3906,  0.1648, -0.3008,  0.7182,  0.7062,  0.0771,  0.4085,  0.7563,\n",
              "         -0.4063, -0.5761,  0.4507, -0.1044,  0.0579,  0.0293, -0.4032, -0.3035,\n",
              "          0.3577,  0.4141,  0.0284, -0.7234, -0.8046, -0.9319, -0.3895, -0.0679,\n",
              "          0.6483, -1.0833, -0.1429,  0.0319, -0.1617,  0.4080,  0.0823, -0.3470,\n",
              "         -0.3136,  0.3890, -0.3417, -1.1319,  0.2788, -0.5906, -1.4668,  0.1593,\n",
              "         -0.3987,  0.4750, -0.0640,  0.3848, -0.3006, -0.1680,  0.4830,  0.1867,\n",
              "         -0.1346, -0.8271,  0.4298, -0.1266,  0.0286,  0.1157,  0.5742,  0.4863,\n",
              "          0.4063,  0.0867, -0.0673,  0.6247, -0.3713, -0.5740,  0.3602,  0.3679,\n",
              "         -0.5224, -0.5025,  0.4599,  0.2808, -0.3272,  0.5404,  0.3917, -0.1459,\n",
              "         -0.3649,  0.5241, -0.0749, -0.4869,  0.0972, -0.1045, -0.7785,  0.2313,\n",
              "          0.8983,  0.3396,  0.2445, -0.1928,  0.2112, -0.2934,  0.7380, -0.4011,\n",
              "          0.2409,  0.0931, -0.2641, -0.6073, -0.2985,  0.6353, -0.4085,  0.0826,\n",
              "          0.8575, -0.3393,  0.6698, -0.1911,  0.2135, -0.4116,  0.3465, -0.4348,\n",
              "         -0.5130,  0.1947, -0.7623, -0.1961, -0.8536,  0.4379,  0.0439,  0.4863,\n",
              "          0.0493,  0.1825, -0.6024, -0.7417, -0.2560, -0.1437, -0.1259,  0.7008,\n",
              "         -0.0763, -0.0045, -0.8196, -0.0269, -0.2504, -0.5709, -0.6732,  0.0775,\n",
              "         -0.0063,  0.2283, -0.7525, -0.2808, -0.6101, -0.1362,  0.1702,  0.1357,\n",
              "         -0.3688,  0.5766, -0.1254,  0.0102,  0.1313, -0.3073,  0.2428, -0.2740,\n",
              "         -0.2777, -0.7507,  1.5319, -0.1997,  0.7639, -0.6499, -0.1212,  0.1430,\n",
              "          0.2970, -0.2317, -0.2895, -0.9113,  0.7781,  0.0568, -0.3318, -0.2592,\n",
              "         -0.4715,  0.1325,  0.1815,  0.3343,  0.1694,  0.5819, -0.4004,  0.4375,\n",
              "         -0.1549,  0.1189, -0.8795, -0.4514,  0.3843,  0.3226, -0.3010, -0.6187,\n",
              "         -0.1506,  0.4193,  0.6569, -0.3780, -0.3249,  0.0032, -0.0243, -0.1052,\n",
              "         -0.2074, -0.5811,  1.2229,  0.5873,  0.2521, -0.1189, -0.4280,  0.1981,\n",
              "          0.3553,  0.1399,  0.0512,  0.4812,  0.0325,  0.0667,  0.1862,  0.9049,\n",
              "         -0.7297, -0.1361,  0.6323, -0.0676,  0.0967, -0.2545,  0.4542, -0.0596,\n",
              "          0.4170,  0.3024, -0.0184, -0.8483, -0.5973,  0.2878,  0.4913, -0.5198,\n",
              "         -0.0514, -0.0935, -0.2566, -0.0058, -0.1104, -0.0950,  0.0388, -0.5833,\n",
              "         -0.5581,  0.4602,  0.6745, -0.3216,  0.2531,  0.1106,  0.2032,  0.8235,\n",
              "          0.4777, -0.0652, -0.4470, -0.2506, -0.2054,  0.4757,  0.1174, -0.2818,\n",
              "         -0.3292, -0.2844, -0.7210,  0.3203, -0.5167,  0.2401, -0.2719,  0.3550,\n",
              "          0.4707, -0.0992, -0.3644, -0.3090, -0.0430,  0.8444,  0.3254, -0.3877,\n",
              "         -0.1627, -0.2180, -0.3532,  0.0102,  0.6608, -0.1040, -0.7325,  0.2966,\n",
              "          0.0857,  0.0055, -0.3310, -0.7356, -0.5433,  0.6103,  0.3443,  0.5841,\n",
              "          0.0558, -0.0943, -0.3444, -0.4107, -0.6642,  0.1279,  0.4710,  0.6307,\n",
              "         -0.3440,  0.0566, -0.2512, -0.3235, -0.0271,  0.2006, -0.5380]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp_OX0CBSjo1",
        "outputId": "a13f57bc-afd5-4b67-ced6-dae675db9115"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "attn_weight_layer = nn.Linear(256 * 2, 295).to(device)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_weights\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0032, 0.0032, 0.0007, 0.0044, 0.0016, 0.0027, 0.0038, 0.0047, 0.0047,\n",
              "         0.0016, 0.0072, 0.0034, 0.0023, 0.0016, 0.0030, 0.0027, 0.0038, 0.0100,\n",
              "         0.0040, 0.0014, 0.0015, 0.0021, 0.0030, 0.0048, 0.0029, 0.0015, 0.0030,\n",
              "         0.0040, 0.0014, 0.0014, 0.0030, 0.0032, 0.0044, 0.0023, 0.0039, 0.0026,\n",
              "         0.0016, 0.0015, 0.0034, 0.0107, 0.0047, 0.0017, 0.0039, 0.0025, 0.0058,\n",
              "         0.0015, 0.0035, 0.0025, 0.0024, 0.0033, 0.0051, 0.0079, 0.0025, 0.0034,\n",
              "         0.0018, 0.0018, 0.0014, 0.0031, 0.0035, 0.0020, 0.0024, 0.0036, 0.0050,\n",
              "         0.0041, 0.0033, 0.0041, 0.0048, 0.0036, 0.0044, 0.0029, 0.0037, 0.0021,\n",
              "         0.0037, 0.0030, 0.0073, 0.0056, 0.0063, 0.0040, 0.0026, 0.0024, 0.0028,\n",
              "         0.0046, 0.0031, 0.0017, 0.0027, 0.0031, 0.0029, 0.0067, 0.0031, 0.0052,\n",
              "         0.0035, 0.0023, 0.0023, 0.0027, 0.0020, 0.0020, 0.0046, 0.0016, 0.0030,\n",
              "         0.0018, 0.0016, 0.0018, 0.0014, 0.0020, 0.0032, 0.0039, 0.0042, 0.0020,\n",
              "         0.0049, 0.0052, 0.0043, 0.0057, 0.0020, 0.0045, 0.0034, 0.0014, 0.0041,\n",
              "         0.0028, 0.0012, 0.0020, 0.0027, 0.0033, 0.0035, 0.0023, 0.0028, 0.0034,\n",
              "         0.0016, 0.0035, 0.0024, 0.0048, 0.0048, 0.0047, 0.0025, 0.0015, 0.0035,\n",
              "         0.0031, 0.0025, 0.0023, 0.0043, 0.0026, 0.0033, 0.0016, 0.0054, 0.0027,\n",
              "         0.0025, 0.0016, 0.0015, 0.0057, 0.0066, 0.0041, 0.0046, 0.0021, 0.0025,\n",
              "         0.0027, 0.0027, 0.0051, 0.0023, 0.0026, 0.0038, 0.0026, 0.0054, 0.0038,\n",
              "         0.0070, 0.0030, 0.0013, 0.0050, 0.0044, 0.0025, 0.0022, 0.0023, 0.0017,\n",
              "         0.0032, 0.0053, 0.0032, 0.0025, 0.0073, 0.0045, 0.0026, 0.0018, 0.0029,\n",
              "         0.0026, 0.0060, 0.0047, 0.0028, 0.0020, 0.0024, 0.0020, 0.0017, 0.0035,\n",
              "         0.0023, 0.0025, 0.0017, 0.0038, 0.0018, 0.0060, 0.0028, 0.0035, 0.0046,\n",
              "         0.0030, 0.0021, 0.0035, 0.0037, 0.0047, 0.0035, 0.0017, 0.0018, 0.0017,\n",
              "         0.0070, 0.0040, 0.0028, 0.0022, 0.0019, 0.0076, 0.0036, 0.0029, 0.0017,\n",
              "         0.0020, 0.0019, 0.0065, 0.0037, 0.0071, 0.0016, 0.0050, 0.0023, 0.0053,\n",
              "         0.0016, 0.0027, 0.0010, 0.0054, 0.0041, 0.0041, 0.0066, 0.0014, 0.0019,\n",
              "         0.0033, 0.0053, 0.0023, 0.0044, 0.0039, 0.0032, 0.0057, 0.0018, 0.0027,\n",
              "         0.0032, 0.0040, 0.0020, 0.0061, 0.0016, 0.0042, 0.0034, 0.0055, 0.0021,\n",
              "         0.0020, 0.0065, 0.0016, 0.0043, 0.0036, 0.0056, 0.0033, 0.0030, 0.0021,\n",
              "         0.0048, 0.0026, 0.0036, 0.0014, 0.0026, 0.0048, 0.0114, 0.0032, 0.0036,\n",
              "         0.0064, 0.0034, 0.0024, 0.0034, 0.0026, 0.0042, 0.0019, 0.0040, 0.0027,\n",
              "         0.0011, 0.0032, 0.0039, 0.0026, 0.0040, 0.0021, 0.0058, 0.0028, 0.0057,\n",
              "         0.0027, 0.0027, 0.0039, 0.0035, 0.0029, 0.0040, 0.0037]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXD9hbQXSme9",
        "outputId": "3e55f1a1-de79-46e0-ac26-ac78816e9aa5"
      },
      "source": [
        "attn_weights.shape, encoder_outputs.shape"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 295]), torch.Size([295, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "FVrmPmxgTL_c",
        "outputId": "3b3811d0-6f45-4de3-be45-4c9d2abed94f"
      },
      "source": [
        "attn_applied = torch.bmm(attn_weights, encoder_outputs)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-740d02d49001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattn_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional tensor, but got 2-dimensional tensor for argument #1 'batch1' (while checking arguments for bmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS_szUYNTnDF"
      },
      "source": [
        "attn_weights.unsqueeze(0).shape, encoder_outputs.unsqueeze(0).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guvc1vEyTvCb",
        "outputId": "d4b7fa2d-448f-42eb-b00e-fb0bf777ed86"
      },
      "source": [
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "attn_applied.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b_2JHkmT_Uz"
      },
      "source": [
        "So, now we have this 256dm attn_applied encoder_outputs capturing what we should focus on on this step. We also have the input we already generated. That's 256dm again. GRU is gonna take 256 only. So we need to concatenate them, send to a linear layer to reduce dimensions, and then send to Gru\n",
        "![image](https://static.wikia.nocookie.net/mycun-the-movie/images/c/c2/Gru-icon.png/revision/latest/scale-to-width-down/250?cb=20151223171656)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oznlhn7tT1tt",
        "outputId": "ee9b03cd-5086-4b79-d9a7-568035b66dff"
      },
      "source": [
        "input_to_gru_layer = nn.Linear(256 * 2, 256).to(device)\n",
        "embedded.shape, attn_applied.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x89whyZIUqBN",
        "outputId": "8c6f36c4-a9c7-4b41-f04a-c9dcd5fe54f3"
      },
      "source": [
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOWvOfQfUvpl",
        "outputId": "91122192-c225-4ea6-cbe8-3180f8c1b995"
      },
      "source": [
        "gru = nn.GRU(256, 256).to(device)\n",
        "decoder_hidden.shape, input_to_gru.shape"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGCflmkaU3BT",
        "outputId": "3696a690-4612-401c-bb30-b30c8ff9dc05"
      },
      "source": [
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "decoder_hidden.shape, input_to_gru.shape"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK1qEzbwVDKR",
        "outputId": "e5b09f54-a524-43a3-8f61-095a6b4d3fe3"
      },
      "source": [
        "output, decoder_hidden = gru(decoder_hidden, input_to_gru)\n",
        "output.shape, decoder_hidden.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DNEbGsZVKB3"
      },
      "source": [
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPGyQhGZVbUL",
        "outputId": "0a0eda23-c5d4-40de-e4db-b48827b3a413"
      },
      "source": [
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim=1)\n",
        "output.shape, output, output.data.topk(1)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1475]),\n",
              " tensor([[0.0007, 0.0007, 0.0008,  ..., 0.0006, 0.0008, 0.0006]],\n",
              "        grad_fn=<SoftmaxBackward0>),\n",
              " torch.return_types.topk(values=tensor([[0.0009]]), indices=tensor([[637]])))"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Iuhy4OMrVp6d",
        "outputId": "aa60d63c-2386-4384-ae38-459050116eca"
      },
      "source": [
        "topv, topi = output.data.topk(1)\n",
        "output_lang.index2word[topi.item()]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'little'"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R0Ganc48V7uN",
        "outputId": "470d3865-d123-4791-b2af-9f2d08cec256"
      },
      "source": [
        "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "decoder_hidden = hidden #decoder_hidden = encoder_hidden\n",
        "output_size = output_lang.n_words\n",
        "embedding = nn.Embedding(output_size, 256).to(device)\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weight_layer = nn.Linear(256 * 2, 295).to(device)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "input_to_gru_layer = nn.Linear(256 * 2, 256).to(device)\n",
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "gru = nn.GRU(256, 256).to(device)\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)\n",
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_index = output.data.topk(1)\n",
        "output_lang.index2word[top_index.item()]\n",
        "\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'brand'"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxTova29WjJG",
        "outputId": "d8c11463-3da9-46c7-8d9e-047e97633ebe"
      },
      "source": [
        "embedding = nn.Embedding(output_size, 256).to(device)\n",
        "attn_weight_layer = nn.Linear(256 * 2, 295).to(device)\n",
        "input_to_gru_layer = nn.Linear(256 * 2, 256).to(device)\n",
        "gru = nn.GRU(256, 256).to(device)\n",
        "output_word_layer = nn.Linear(256, output_lang.n_words).to(device)\n",
        "\n",
        "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "decoder_hidden = hidden\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_index = output.data.topk(1)\n",
        "output_lang.index2word[top_index.item()], attn_weights"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('french',\n",
              " tensor([[0.0045, 0.0033, 0.0013, 0.0028, 0.0028, 0.0024, 0.0020, 0.0017, 0.0081,\n",
              "          0.0035, 0.0025, 0.0038, 0.0067, 0.0029, 0.0043, 0.0037, 0.0083, 0.0044,\n",
              "          0.0034, 0.0015, 0.0027, 0.0019, 0.0026, 0.0032, 0.0034, 0.0027, 0.0038,\n",
              "          0.0016, 0.0027, 0.0056, 0.0052, 0.0031, 0.0023, 0.0039, 0.0029, 0.0048,\n",
              "          0.0042, 0.0033, 0.0020, 0.0044, 0.0031, 0.0053, 0.0013, 0.0033, 0.0055,\n",
              "          0.0048, 0.0022, 0.0027, 0.0016, 0.0044, 0.0015, 0.0027, 0.0032, 0.0078,\n",
              "          0.0056, 0.0012, 0.0052, 0.0041, 0.0036, 0.0028, 0.0029, 0.0049, 0.0028,\n",
              "          0.0046, 0.0043, 0.0040, 0.0031, 0.0033, 0.0043, 0.0031, 0.0054, 0.0021,\n",
              "          0.0024, 0.0015, 0.0010, 0.0050, 0.0019, 0.0016, 0.0055, 0.0017, 0.0017,\n",
              "          0.0023, 0.0025, 0.0024, 0.0024, 0.0021, 0.0035, 0.0025, 0.0059, 0.0062,\n",
              "          0.0047, 0.0008, 0.0015, 0.0028, 0.0014, 0.0023, 0.0051, 0.0021, 0.0027,\n",
              "          0.0015, 0.0022, 0.0021, 0.0045, 0.0028, 0.0028, 0.0039, 0.0027, 0.0024,\n",
              "          0.0056, 0.0021, 0.0024, 0.0040, 0.0034, 0.0029, 0.0025, 0.0038, 0.0031,\n",
              "          0.0027, 0.0023, 0.0039, 0.0029, 0.0021, 0.0033, 0.0043, 0.0050, 0.0021,\n",
              "          0.0068, 0.0025, 0.0068, 0.0014, 0.0034, 0.0036, 0.0023, 0.0048, 0.0028,\n",
              "          0.0039, 0.0041, 0.0023, 0.0048, 0.0033, 0.0036, 0.0029, 0.0021, 0.0044,\n",
              "          0.0028, 0.0035, 0.0021, 0.0027, 0.0030, 0.0022, 0.0042, 0.0049, 0.0023,\n",
              "          0.0023, 0.0050, 0.0036, 0.0035, 0.0066, 0.0025, 0.0026, 0.0019, 0.0049,\n",
              "          0.0038, 0.0022, 0.0043, 0.0044, 0.0041, 0.0061, 0.0026, 0.0024, 0.0057,\n",
              "          0.0062, 0.0041, 0.0012, 0.0024, 0.0058, 0.0024, 0.0041, 0.0022, 0.0042,\n",
              "          0.0026, 0.0015, 0.0024, 0.0056, 0.0055, 0.0022, 0.0032, 0.0023, 0.0029,\n",
              "          0.0029, 0.0033, 0.0025, 0.0042, 0.0036, 0.0024, 0.0046, 0.0043, 0.0032,\n",
              "          0.0022, 0.0018, 0.0018, 0.0043, 0.0029, 0.0023, 0.0036, 0.0032, 0.0032,\n",
              "          0.0033, 0.0039, 0.0024, 0.0024, 0.0018, 0.0035, 0.0033, 0.0025, 0.0064,\n",
              "          0.0043, 0.0028, 0.0043, 0.0018, 0.0026, 0.0042, 0.0028, 0.0046, 0.0035,\n",
              "          0.0028, 0.0027, 0.0032, 0.0059, 0.0025, 0.0026, 0.0027, 0.0028, 0.0026,\n",
              "          0.0028, 0.0019, 0.0015, 0.0035, 0.0051, 0.0018, 0.0033, 0.0046, 0.0023,\n",
              "          0.0020, 0.0016, 0.0017, 0.0054, 0.0017, 0.0019, 0.0016, 0.0038, 0.0008,\n",
              "          0.0032, 0.0039, 0.0047, 0.0046, 0.0064, 0.0025, 0.0053, 0.0024, 0.0016,\n",
              "          0.0063, 0.0023, 0.0079, 0.0035, 0.0045, 0.0040, 0.0022, 0.0029, 0.0016,\n",
              "          0.0032, 0.0047, 0.0080, 0.0013, 0.0074, 0.0036, 0.0011, 0.0054, 0.0028,\n",
              "          0.0016, 0.0063, 0.0043, 0.0045, 0.0026, 0.0047, 0.0042, 0.0027, 0.0043,\n",
              "          0.0051, 0.0033, 0.0039, 0.0031, 0.0034, 0.0045, 0.0037]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNs6bXVvWvEp",
        "outputId": "b3bba990-6e9d-49c9-ad2d-9bd8a9072bf5"
      },
      "source": [
        "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "decoder_hidden = hidden\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_index = output.data.topk(1)\n",
        "output_lang.index2word[top_index.item()], attn_weights"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('french',\n",
              " tensor([[0.0045, 0.0033, 0.0013, 0.0028, 0.0028, 0.0024, 0.0020, 0.0017, 0.0081,\n",
              "          0.0035, 0.0025, 0.0038, 0.0067, 0.0029, 0.0043, 0.0037, 0.0083, 0.0044,\n",
              "          0.0034, 0.0015, 0.0027, 0.0019, 0.0026, 0.0032, 0.0034, 0.0027, 0.0038,\n",
              "          0.0016, 0.0027, 0.0056, 0.0052, 0.0031, 0.0023, 0.0039, 0.0029, 0.0048,\n",
              "          0.0042, 0.0033, 0.0020, 0.0044, 0.0031, 0.0053, 0.0013, 0.0033, 0.0055,\n",
              "          0.0048, 0.0022, 0.0027, 0.0016, 0.0044, 0.0015, 0.0027, 0.0032, 0.0078,\n",
              "          0.0056, 0.0012, 0.0052, 0.0041, 0.0036, 0.0028, 0.0029, 0.0049, 0.0028,\n",
              "          0.0046, 0.0043, 0.0040, 0.0031, 0.0033, 0.0043, 0.0031, 0.0054, 0.0021,\n",
              "          0.0024, 0.0015, 0.0010, 0.0050, 0.0019, 0.0016, 0.0055, 0.0017, 0.0017,\n",
              "          0.0023, 0.0025, 0.0024, 0.0024, 0.0021, 0.0035, 0.0025, 0.0059, 0.0062,\n",
              "          0.0047, 0.0008, 0.0015, 0.0028, 0.0014, 0.0023, 0.0051, 0.0021, 0.0027,\n",
              "          0.0015, 0.0022, 0.0021, 0.0045, 0.0028, 0.0028, 0.0039, 0.0027, 0.0024,\n",
              "          0.0056, 0.0021, 0.0024, 0.0040, 0.0034, 0.0029, 0.0025, 0.0038, 0.0031,\n",
              "          0.0027, 0.0023, 0.0039, 0.0029, 0.0021, 0.0033, 0.0043, 0.0050, 0.0021,\n",
              "          0.0068, 0.0025, 0.0068, 0.0014, 0.0034, 0.0036, 0.0023, 0.0048, 0.0028,\n",
              "          0.0039, 0.0041, 0.0023, 0.0048, 0.0033, 0.0036, 0.0029, 0.0021, 0.0044,\n",
              "          0.0028, 0.0035, 0.0021, 0.0027, 0.0030, 0.0022, 0.0042, 0.0049, 0.0023,\n",
              "          0.0023, 0.0050, 0.0036, 0.0035, 0.0066, 0.0025, 0.0026, 0.0019, 0.0049,\n",
              "          0.0038, 0.0022, 0.0043, 0.0044, 0.0041, 0.0061, 0.0026, 0.0024, 0.0057,\n",
              "          0.0062, 0.0041, 0.0012, 0.0024, 0.0058, 0.0024, 0.0041, 0.0022, 0.0042,\n",
              "          0.0026, 0.0015, 0.0024, 0.0056, 0.0055, 0.0022, 0.0032, 0.0023, 0.0029,\n",
              "          0.0029, 0.0033, 0.0025, 0.0042, 0.0036, 0.0024, 0.0046, 0.0043, 0.0032,\n",
              "          0.0022, 0.0018, 0.0018, 0.0043, 0.0029, 0.0023, 0.0036, 0.0032, 0.0032,\n",
              "          0.0033, 0.0039, 0.0024, 0.0024, 0.0018, 0.0035, 0.0033, 0.0025, 0.0064,\n",
              "          0.0043, 0.0028, 0.0043, 0.0018, 0.0026, 0.0042, 0.0028, 0.0046, 0.0035,\n",
              "          0.0028, 0.0027, 0.0032, 0.0059, 0.0025, 0.0026, 0.0027, 0.0028, 0.0026,\n",
              "          0.0028, 0.0019, 0.0015, 0.0035, 0.0051, 0.0018, 0.0033, 0.0046, 0.0023,\n",
              "          0.0020, 0.0016, 0.0017, 0.0054, 0.0017, 0.0019, 0.0016, 0.0038, 0.0008,\n",
              "          0.0032, 0.0039, 0.0047, 0.0046, 0.0064, 0.0025, 0.0053, 0.0024, 0.0016,\n",
              "          0.0063, 0.0023, 0.0079, 0.0035, 0.0045, 0.0040, 0.0022, 0.0029, 0.0016,\n",
              "          0.0032, 0.0047, 0.0080, 0.0013, 0.0074, 0.0036, 0.0011, 0.0054, 0.0028,\n",
              "          0.0016, 0.0063, 0.0043, 0.0045, 0.0026, 0.0047, 0.0042, 0.0027, 0.0043,\n",
              "          0.0051, 0.0033, 0.0039, 0.0031, 0.0034, 0.0045, 0.0037]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZu1L1GCXIgs",
        "outputId": "f346806b-d087-4163-968b-b251567d3cd2"
      },
      "source": [
        "decoder_input = torch.tensor([[top_index.item()]], device=device)\n",
        "decoder_hidden = hidden\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_index = output.data.topk(1)\n",
        "output_lang.index2word[top_index.item()], attn_weights"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('morning',\n",
              " tensor([[0.0028, 0.0074, 0.0022, 0.0016, 0.0017, 0.0055, 0.0044, 0.0017, 0.0058,\n",
              "          0.0020, 0.0023, 0.0034, 0.0029, 0.0043, 0.0024, 0.0025, 0.0029, 0.0029,\n",
              "          0.0046, 0.0039, 0.0010, 0.0029, 0.0021, 0.0026, 0.0036, 0.0033, 0.0023,\n",
              "          0.0033, 0.0044, 0.0028, 0.0023, 0.0038, 0.0021, 0.0043, 0.0048, 0.0046,\n",
              "          0.0054, 0.0029, 0.0020, 0.0021, 0.0031, 0.0025, 0.0013, 0.0067, 0.0016,\n",
              "          0.0056, 0.0024, 0.0061, 0.0033, 0.0024, 0.0038, 0.0037, 0.0043, 0.0037,\n",
              "          0.0036, 0.0045, 0.0027, 0.0037, 0.0040, 0.0046, 0.0044, 0.0018, 0.0041,\n",
              "          0.0028, 0.0066, 0.0032, 0.0021, 0.0028, 0.0026, 0.0022, 0.0041, 0.0013,\n",
              "          0.0063, 0.0012, 0.0015, 0.0026, 0.0028, 0.0045, 0.0021, 0.0031, 0.0049,\n",
              "          0.0021, 0.0019, 0.0046, 0.0059, 0.0057, 0.0044, 0.0051, 0.0046, 0.0023,\n",
              "          0.0018, 0.0018, 0.0062, 0.0023, 0.0048, 0.0061, 0.0010, 0.0049, 0.0044,\n",
              "          0.0040, 0.0029, 0.0038, 0.0035, 0.0023, 0.0016, 0.0038, 0.0024, 0.0039,\n",
              "          0.0060, 0.0020, 0.0020, 0.0026, 0.0046, 0.0061, 0.0022, 0.0021, 0.0036,\n",
              "          0.0033, 0.0026, 0.0039, 0.0040, 0.0028, 0.0031, 0.0057, 0.0024, 0.0048,\n",
              "          0.0040, 0.0028, 0.0020, 0.0036, 0.0023, 0.0026, 0.0008, 0.0018, 0.0034,\n",
              "          0.0024, 0.0033, 0.0044, 0.0022, 0.0022, 0.0056, 0.0058, 0.0017, 0.0032,\n",
              "          0.0016, 0.0046, 0.0021, 0.0012, 0.0097, 0.0042, 0.0029, 0.0041, 0.0024,\n",
              "          0.0027, 0.0067, 0.0017, 0.0037, 0.0054, 0.0042, 0.0059, 0.0036, 0.0086,\n",
              "          0.0041, 0.0025, 0.0043, 0.0026, 0.0044, 0.0049, 0.0043, 0.0033, 0.0050,\n",
              "          0.0027, 0.0015, 0.0024, 0.0019, 0.0063, 0.0035, 0.0066, 0.0021, 0.0028,\n",
              "          0.0053, 0.0032, 0.0026, 0.0028, 0.0031, 0.0017, 0.0018, 0.0023, 0.0025,\n",
              "          0.0025, 0.0065, 0.0026, 0.0017, 0.0037, 0.0029, 0.0018, 0.0030, 0.0022,\n",
              "          0.0046, 0.0024, 0.0030, 0.0068, 0.0122, 0.0034, 0.0040, 0.0034, 0.0037,\n",
              "          0.0050, 0.0038, 0.0026, 0.0020, 0.0035, 0.0029, 0.0025, 0.0028, 0.0032,\n",
              "          0.0036, 0.0090, 0.0045, 0.0034, 0.0025, 0.0049, 0.0046, 0.0022, 0.0014,\n",
              "          0.0016, 0.0021, 0.0032, 0.0028, 0.0026, 0.0041, 0.0033, 0.0028, 0.0016,\n",
              "          0.0019, 0.0012, 0.0029, 0.0030, 0.0025, 0.0024, 0.0019, 0.0037, 0.0041,\n",
              "          0.0041, 0.0038, 0.0031, 0.0020, 0.0026, 0.0030, 0.0030, 0.0019, 0.0025,\n",
              "          0.0025, 0.0060, 0.0022, 0.0020, 0.0039, 0.0020, 0.0037, 0.0022, 0.0034,\n",
              "          0.0028, 0.0046, 0.0024, 0.0060, 0.0023, 0.0028, 0.0029, 0.0032, 0.0024,\n",
              "          0.0056, 0.0062, 0.0029, 0.0025, 0.0074, 0.0021, 0.0054, 0.0037, 0.0024,\n",
              "          0.0040, 0.0022, 0.0023, 0.0040, 0.0021, 0.0026, 0.0024, 0.0041, 0.0019,\n",
              "          0.0024, 0.0029, 0.0020, 0.0027, 0.0036, 0.0017, 0.0029]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZFrigonXdYp",
        "outputId": "bba65eee-c43e-4a63-d518-a9509389e635"
      },
      "source": [
        "decoder_input = torch.tensor([[top_index.item()]], device=device)\n",
        "decoder_hidden = hidden\n",
        "output_size = output_lang.n_words\n",
        "embedded = embedding(decoder_input)\n",
        "attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "input_to_gru = input_to_gru.unsqueeze(0)\n",
        "output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "output = F.relu(output)\n",
        "output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "top_value, top_index = output.data.topk(1)\n",
        "output_lang.index2word[top_index.item()], attn_weights"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('same',\n",
              " tensor([[0.0046, 0.0020, 0.0036, 0.0025, 0.0033, 0.0024, 0.0030, 0.0041, 0.0030,\n",
              "          0.0024, 0.0013, 0.0053, 0.0046, 0.0031, 0.0043, 0.0025, 0.0023, 0.0035,\n",
              "          0.0041, 0.0027, 0.0047, 0.0026, 0.0075, 0.0036, 0.0023, 0.0019, 0.0024,\n",
              "          0.0029, 0.0024, 0.0054, 0.0050, 0.0039, 0.0024, 0.0021, 0.0034, 0.0021,\n",
              "          0.0052, 0.0029, 0.0037, 0.0031, 0.0050, 0.0027, 0.0030, 0.0030, 0.0040,\n",
              "          0.0037, 0.0014, 0.0034, 0.0016, 0.0023, 0.0034, 0.0017, 0.0028, 0.0049,\n",
              "          0.0045, 0.0026, 0.0038, 0.0025, 0.0050, 0.0052, 0.0009, 0.0014, 0.0034,\n",
              "          0.0049, 0.0049, 0.0015, 0.0013, 0.0036, 0.0022, 0.0037, 0.0013, 0.0035,\n",
              "          0.0040, 0.0014, 0.0012, 0.0048, 0.0016, 0.0022, 0.0026, 0.0015, 0.0029,\n",
              "          0.0054, 0.0020, 0.0036, 0.0045, 0.0042, 0.0035, 0.0031, 0.0018, 0.0060,\n",
              "          0.0030, 0.0029, 0.0027, 0.0018, 0.0039, 0.0031, 0.0038, 0.0011, 0.0061,\n",
              "          0.0016, 0.0050, 0.0063, 0.0020, 0.0027, 0.0041, 0.0042, 0.0024, 0.0046,\n",
              "          0.0047, 0.0030, 0.0023, 0.0037, 0.0030, 0.0031, 0.0042, 0.0044, 0.0049,\n",
              "          0.0018, 0.0048, 0.0031, 0.0021, 0.0028, 0.0083, 0.0032, 0.0045, 0.0026,\n",
              "          0.0026, 0.0027, 0.0032, 0.0018, 0.0041, 0.0024, 0.0017, 0.0039, 0.0027,\n",
              "          0.0022, 0.0056, 0.0025, 0.0032, 0.0019, 0.0036, 0.0043, 0.0038, 0.0039,\n",
              "          0.0030, 0.0033, 0.0037, 0.0033, 0.0022, 0.0019, 0.0034, 0.0027, 0.0056,\n",
              "          0.0052, 0.0099, 0.0020, 0.0035, 0.0036, 0.0033, 0.0037, 0.0028, 0.0028,\n",
              "          0.0057, 0.0034, 0.0041, 0.0020, 0.0013, 0.0025, 0.0018, 0.0029, 0.0025,\n",
              "          0.0029, 0.0017, 0.0022, 0.0069, 0.0039, 0.0035, 0.0029, 0.0034, 0.0022,\n",
              "          0.0067, 0.0027, 0.0037, 0.0039, 0.0024, 0.0040, 0.0041, 0.0020, 0.0031,\n",
              "          0.0056, 0.0047, 0.0031, 0.0044, 0.0069, 0.0023, 0.0058, 0.0051, 0.0021,\n",
              "          0.0037, 0.0026, 0.0018, 0.0041, 0.0027, 0.0025, 0.0020, 0.0060, 0.0020,\n",
              "          0.0019, 0.0061, 0.0024, 0.0026, 0.0041, 0.0041, 0.0016, 0.0042, 0.0011,\n",
              "          0.0025, 0.0041, 0.0054, 0.0044, 0.0015, 0.0018, 0.0042, 0.0042, 0.0038,\n",
              "          0.0083, 0.0106, 0.0036, 0.0041, 0.0034, 0.0022, 0.0017, 0.0018, 0.0012,\n",
              "          0.0026, 0.0038, 0.0052, 0.0048, 0.0050, 0.0020, 0.0089, 0.0043, 0.0043,\n",
              "          0.0043, 0.0011, 0.0025, 0.0033, 0.0012, 0.0031, 0.0020, 0.0035, 0.0015,\n",
              "          0.0052, 0.0046, 0.0022, 0.0037, 0.0044, 0.0024, 0.0035, 0.0010, 0.0010,\n",
              "          0.0046, 0.0030, 0.0043, 0.0017, 0.0040, 0.0022, 0.0016, 0.0030, 0.0020,\n",
              "          0.0057, 0.0050, 0.0017, 0.0041, 0.0043, 0.0042, 0.0030, 0.0031, 0.0038,\n",
              "          0.0030, 0.0043, 0.0013, 0.0075, 0.0025, 0.0035, 0.0032, 0.0052, 0.0039,\n",
              "          0.0021, 0.0023, 0.0038, 0.0028, 0.0026, 0.0039, 0.0035]],\n",
              "        grad_fn=<SoftmaxBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ep8CM-XfbH",
        "outputId": "a7ffaac1-7cd1-4087-cdac-e20c38d7be26"
      },
      "source": [
        "for i in range(6):\n",
        "  decoder_input = torch.tensor([[output_indices[i]]], device=device)\n",
        "  decoder_hidden = hidden\n",
        "  output_size = output_lang.n_words\n",
        "  embedded = embedding(decoder_input)\n",
        "  attn_weights = attn_weight_layer(torch.cat((embedded[0], decoder_hidden[0]), 1))\n",
        "  attn_weights = F.softmax(attn_weights, dim = 1)\n",
        "  attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "  input_to_gru = input_to_gru_layer(torch.cat((embedded[0], attn_applied[0]), 1))\n",
        "  input_to_gru = input_to_gru.unsqueeze(0)\n",
        "  output, decoder_hidden = gru(input_to_gru, decoder_hidden)\n",
        "  output = F.relu(output)\n",
        "  output = F.softmax(output_word_layer(output[0]), dim = 1)\n",
        "  top_value, top_index = output.data.topk(1)\n",
        "  print(output_sentence.split(\" \")[i], output_indices[i], output_lang.index2word[top_index.item()], top_index.item() )\n",
        "  print(attn_weights)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i 2 excluding 624\n",
            "tensor([[0.0013, 0.0038, 0.0024, 0.0021, 0.0026, 0.0025, 0.0041, 0.0021, 0.0027,\n",
            "         0.0018, 0.0026, 0.0054, 0.0027, 0.0027, 0.0051, 0.0040, 0.0029, 0.0034,\n",
            "         0.0042, 0.0019, 0.0029, 0.0048, 0.0041, 0.0025, 0.0043, 0.0046, 0.0030,\n",
            "         0.0030, 0.0048, 0.0039, 0.0027, 0.0031, 0.0041, 0.0043, 0.0036, 0.0062,\n",
            "         0.0038, 0.0056, 0.0038, 0.0034, 0.0036, 0.0051, 0.0024, 0.0021, 0.0017,\n",
            "         0.0046, 0.0014, 0.0017, 0.0021, 0.0031, 0.0033, 0.0029, 0.0024, 0.0033,\n",
            "         0.0034, 0.0018, 0.0015, 0.0049, 0.0031, 0.0020, 0.0046, 0.0033, 0.0043,\n",
            "         0.0058, 0.0043, 0.0024, 0.0028, 0.0024, 0.0046, 0.0040, 0.0038, 0.0024,\n",
            "         0.0029, 0.0022, 0.0022, 0.0028, 0.0016, 0.0035, 0.0038, 0.0048, 0.0023,\n",
            "         0.0040, 0.0028, 0.0039, 0.0054, 0.0015, 0.0074, 0.0022, 0.0027, 0.0046,\n",
            "         0.0045, 0.0050, 0.0020, 0.0032, 0.0044, 0.0030, 0.0021, 0.0035, 0.0033,\n",
            "         0.0027, 0.0011, 0.0042, 0.0023, 0.0023, 0.0020, 0.0036, 0.0064, 0.0038,\n",
            "         0.0020, 0.0023, 0.0036, 0.0032, 0.0058, 0.0064, 0.0041, 0.0036, 0.0041,\n",
            "         0.0024, 0.0027, 0.0059, 0.0036, 0.0035, 0.0033, 0.0043, 0.0029, 0.0033,\n",
            "         0.0028, 0.0039, 0.0037, 0.0023, 0.0057, 0.0026, 0.0028, 0.0047, 0.0036,\n",
            "         0.0025, 0.0036, 0.0048, 0.0035, 0.0030, 0.0038, 0.0031, 0.0017, 0.0032,\n",
            "         0.0025, 0.0025, 0.0041, 0.0017, 0.0030, 0.0027, 0.0034, 0.0048, 0.0023,\n",
            "         0.0030, 0.0058, 0.0042, 0.0054, 0.0038, 0.0041, 0.0038, 0.0020, 0.0021,\n",
            "         0.0028, 0.0031, 0.0041, 0.0042, 0.0019, 0.0041, 0.0027, 0.0053, 0.0031,\n",
            "         0.0036, 0.0023, 0.0032, 0.0021, 0.0036, 0.0035, 0.0043, 0.0042, 0.0046,\n",
            "         0.0031, 0.0024, 0.0055, 0.0034, 0.0028, 0.0016, 0.0040, 0.0019, 0.0026,\n",
            "         0.0017, 0.0033, 0.0026, 0.0050, 0.0020, 0.0013, 0.0038, 0.0031, 0.0024,\n",
            "         0.0017, 0.0051, 0.0044, 0.0071, 0.0021, 0.0024, 0.0032, 0.0024, 0.0030,\n",
            "         0.0023, 0.0032, 0.0056, 0.0028, 0.0030, 0.0052, 0.0059, 0.0037, 0.0016,\n",
            "         0.0039, 0.0029, 0.0033, 0.0028, 0.0047, 0.0036, 0.0031, 0.0032, 0.0018,\n",
            "         0.0061, 0.0023, 0.0027, 0.0025, 0.0021, 0.0042, 0.0020, 0.0030, 0.0029,\n",
            "         0.0021, 0.0041, 0.0033, 0.0033, 0.0042, 0.0044, 0.0041, 0.0046, 0.0023,\n",
            "         0.0015, 0.0049, 0.0032, 0.0031, 0.0021, 0.0019, 0.0041, 0.0051, 0.0029,\n",
            "         0.0045, 0.0062, 0.0020, 0.0025, 0.0026, 0.0034, 0.0018, 0.0055, 0.0029,\n",
            "         0.0038, 0.0025, 0.0023, 0.0072, 0.0018, 0.0018, 0.0035, 0.0040, 0.0038,\n",
            "         0.0034, 0.0025, 0.0049, 0.0042, 0.0047, 0.0039, 0.0017, 0.0051, 0.0039,\n",
            "         0.0080, 0.0028, 0.0017, 0.0023, 0.0032, 0.0015, 0.0044, 0.0023, 0.0019,\n",
            "         0.0061, 0.0019, 0.0030, 0.0078, 0.0024, 0.0044, 0.0026]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "am 21 french 1431\n",
            "tensor([[0.0050, 0.0027, 0.0026, 0.0030, 0.0038, 0.0047, 0.0070, 0.0022, 0.0054,\n",
            "         0.0031, 0.0061, 0.0025, 0.0037, 0.0035, 0.0018, 0.0034, 0.0044, 0.0023,\n",
            "         0.0035, 0.0021, 0.0030, 0.0015, 0.0047, 0.0029, 0.0034, 0.0025, 0.0025,\n",
            "         0.0023, 0.0018, 0.0029, 0.0031, 0.0067, 0.0013, 0.0049, 0.0039, 0.0038,\n",
            "         0.0062, 0.0021, 0.0023, 0.0025, 0.0013, 0.0031, 0.0042, 0.0046, 0.0036,\n",
            "         0.0049, 0.0023, 0.0042, 0.0027, 0.0018, 0.0037, 0.0027, 0.0022, 0.0096,\n",
            "         0.0027, 0.0023, 0.0043, 0.0025, 0.0042, 0.0059, 0.0025, 0.0040, 0.0021,\n",
            "         0.0032, 0.0048, 0.0043, 0.0017, 0.0040, 0.0017, 0.0047, 0.0046, 0.0014,\n",
            "         0.0037, 0.0047, 0.0021, 0.0041, 0.0032, 0.0045, 0.0026, 0.0022, 0.0023,\n",
            "         0.0027, 0.0020, 0.0032, 0.0088, 0.0022, 0.0027, 0.0032, 0.0056, 0.0036,\n",
            "         0.0041, 0.0033, 0.0021, 0.0027, 0.0041, 0.0028, 0.0021, 0.0034, 0.0019,\n",
            "         0.0048, 0.0031, 0.0037, 0.0029, 0.0022, 0.0026, 0.0028, 0.0069, 0.0026,\n",
            "         0.0031, 0.0048, 0.0020, 0.0031, 0.0044, 0.0020, 0.0043, 0.0027, 0.0011,\n",
            "         0.0041, 0.0025, 0.0035, 0.0017, 0.0036, 0.0033, 0.0057, 0.0055, 0.0039,\n",
            "         0.0020, 0.0043, 0.0027, 0.0051, 0.0028, 0.0026, 0.0025, 0.0048, 0.0027,\n",
            "         0.0018, 0.0035, 0.0059, 0.0017, 0.0023, 0.0055, 0.0031, 0.0018, 0.0031,\n",
            "         0.0028, 0.0025, 0.0033, 0.0033, 0.0049, 0.0062, 0.0023, 0.0020, 0.0030,\n",
            "         0.0013, 0.0045, 0.0028, 0.0063, 0.0045, 0.0031, 0.0034, 0.0022, 0.0017,\n",
            "         0.0027, 0.0028, 0.0021, 0.0025, 0.0025, 0.0059, 0.0035, 0.0029, 0.0031,\n",
            "         0.0055, 0.0044, 0.0043, 0.0036, 0.0085, 0.0028, 0.0052, 0.0094, 0.0029,\n",
            "         0.0061, 0.0021, 0.0090, 0.0056, 0.0039, 0.0045, 0.0017, 0.0019, 0.0042,\n",
            "         0.0047, 0.0042, 0.0053, 0.0011, 0.0057, 0.0017, 0.0017, 0.0056, 0.0011,\n",
            "         0.0044, 0.0048, 0.0028, 0.0044, 0.0106, 0.0019, 0.0026, 0.0014, 0.0023,\n",
            "         0.0014, 0.0028, 0.0031, 0.0012, 0.0046, 0.0036, 0.0030, 0.0044, 0.0016,\n",
            "         0.0038, 0.0029, 0.0020, 0.0022, 0.0009, 0.0048, 0.0030, 0.0021, 0.0035,\n",
            "         0.0019, 0.0039, 0.0024, 0.0011, 0.0021, 0.0027, 0.0044, 0.0056, 0.0021,\n",
            "         0.0033, 0.0022, 0.0023, 0.0055, 0.0018, 0.0032, 0.0032, 0.0032, 0.0031,\n",
            "         0.0025, 0.0037, 0.0024, 0.0018, 0.0017, 0.0054, 0.0021, 0.0017, 0.0049,\n",
            "         0.0029, 0.0030, 0.0053, 0.0023, 0.0029, 0.0018, 0.0026, 0.0034, 0.0040,\n",
            "         0.0059, 0.0018, 0.0014, 0.0036, 0.0024, 0.0018, 0.0017, 0.0034, 0.0037,\n",
            "         0.0029, 0.0021, 0.0030, 0.0034, 0.0042, 0.0047, 0.0043, 0.0019, 0.0024,\n",
            "         0.0029, 0.0039, 0.0023, 0.0021, 0.0032, 0.0026, 0.0049, 0.0064, 0.0037,\n",
            "         0.0042, 0.0044, 0.0040, 0.0026, 0.0026, 0.0030, 0.0033]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "a 29 curious 244\n",
            "tensor([[0.0035, 0.0024, 0.0057, 0.0035, 0.0027, 0.0022, 0.0025, 0.0062, 0.0066,\n",
            "         0.0060, 0.0015, 0.0042, 0.0020, 0.0025, 0.0031, 0.0028, 0.0031, 0.0013,\n",
            "         0.0032, 0.0066, 0.0025, 0.0034, 0.0032, 0.0015, 0.0026, 0.0040, 0.0020,\n",
            "         0.0025, 0.0068, 0.0019, 0.0082, 0.0033, 0.0041, 0.0062, 0.0032, 0.0025,\n",
            "         0.0012, 0.0032, 0.0027, 0.0032, 0.0025, 0.0027, 0.0039, 0.0021, 0.0034,\n",
            "         0.0045, 0.0029, 0.0048, 0.0032, 0.0057, 0.0013, 0.0018, 0.0022, 0.0036,\n",
            "         0.0043, 0.0038, 0.0039, 0.0073, 0.0021, 0.0017, 0.0072, 0.0014, 0.0021,\n",
            "         0.0012, 0.0047, 0.0034, 0.0064, 0.0035, 0.0022, 0.0016, 0.0012, 0.0031,\n",
            "         0.0047, 0.0024, 0.0020, 0.0024, 0.0032, 0.0037, 0.0023, 0.0018, 0.0024,\n",
            "         0.0048, 0.0046, 0.0029, 0.0031, 0.0037, 0.0041, 0.0012, 0.0018, 0.0007,\n",
            "         0.0028, 0.0016, 0.0028, 0.0060, 0.0038, 0.0035, 0.0020, 0.0019, 0.0078,\n",
            "         0.0025, 0.0029, 0.0019, 0.0024, 0.0041, 0.0070, 0.0019, 0.0018, 0.0038,\n",
            "         0.0017, 0.0058, 0.0057, 0.0017, 0.0027, 0.0017, 0.0051, 0.0021, 0.0037,\n",
            "         0.0042, 0.0032, 0.0030, 0.0018, 0.0022, 0.0023, 0.0050, 0.0030, 0.0047,\n",
            "         0.0039, 0.0022, 0.0032, 0.0045, 0.0030, 0.0014, 0.0026, 0.0035, 0.0031,\n",
            "         0.0048, 0.0042, 0.0021, 0.0027, 0.0045, 0.0049, 0.0031, 0.0031, 0.0058,\n",
            "         0.0030, 0.0038, 0.0028, 0.0027, 0.0014, 0.0031, 0.0019, 0.0071, 0.0036,\n",
            "         0.0037, 0.0059, 0.0050, 0.0021, 0.0023, 0.0034, 0.0036, 0.0025, 0.0045,\n",
            "         0.0029, 0.0053, 0.0019, 0.0024, 0.0053, 0.0050, 0.0023, 0.0026, 0.0039,\n",
            "         0.0037, 0.0038, 0.0018, 0.0028, 0.0026, 0.0026, 0.0052, 0.0042, 0.0023,\n",
            "         0.0059, 0.0048, 0.0031, 0.0034, 0.0023, 0.0027, 0.0039, 0.0060, 0.0028,\n",
            "         0.0037, 0.0039, 0.0036, 0.0045, 0.0053, 0.0039, 0.0018, 0.0023, 0.0069,\n",
            "         0.0046, 0.0025, 0.0044, 0.0021, 0.0048, 0.0038, 0.0032, 0.0015, 0.0031,\n",
            "         0.0045, 0.0058, 0.0034, 0.0027, 0.0030, 0.0023, 0.0025, 0.0024, 0.0034,\n",
            "         0.0044, 0.0048, 0.0037, 0.0022, 0.0045, 0.0029, 0.0030, 0.0035, 0.0102,\n",
            "         0.0029, 0.0032, 0.0021, 0.0031, 0.0036, 0.0026, 0.0047, 0.0024, 0.0024,\n",
            "         0.0042, 0.0011, 0.0033, 0.0038, 0.0061, 0.0064, 0.0020, 0.0037, 0.0027,\n",
            "         0.0042, 0.0029, 0.0031, 0.0041, 0.0044, 0.0014, 0.0017, 0.0033, 0.0034,\n",
            "         0.0027, 0.0021, 0.0021, 0.0092, 0.0028, 0.0018, 0.0040, 0.0054, 0.0022,\n",
            "         0.0012, 0.0040, 0.0050, 0.0054, 0.0053, 0.0025, 0.0034, 0.0033, 0.0022,\n",
            "         0.0017, 0.0050, 0.0025, 0.0036, 0.0023, 0.0053, 0.0022, 0.0023, 0.0022,\n",
            "         0.0024, 0.0046, 0.0039, 0.0033, 0.0040, 0.0013, 0.0046, 0.0022, 0.0033,\n",
            "         0.0020, 0.0043, 0.0024, 0.0026, 0.0017, 0.0021, 0.0018]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "year 4 morning 749\n",
            "tensor([[0.0028, 0.0016, 0.0015, 0.0030, 0.0028, 0.0048, 0.0048, 0.0024, 0.0022,\n",
            "         0.0029, 0.0022, 0.0057, 0.0056, 0.0035, 0.0067, 0.0039, 0.0046, 0.0027,\n",
            "         0.0049, 0.0034, 0.0038, 0.0033, 0.0021, 0.0053, 0.0064, 0.0030, 0.0061,\n",
            "         0.0075, 0.0028, 0.0047, 0.0049, 0.0024, 0.0043, 0.0028, 0.0036, 0.0027,\n",
            "         0.0029, 0.0019, 0.0036, 0.0042, 0.0026, 0.0033, 0.0018, 0.0052, 0.0022,\n",
            "         0.0023, 0.0028, 0.0047, 0.0044, 0.0035, 0.0037, 0.0016, 0.0038, 0.0066,\n",
            "         0.0024, 0.0022, 0.0041, 0.0032, 0.0021, 0.0036, 0.0020, 0.0029, 0.0074,\n",
            "         0.0036, 0.0034, 0.0026, 0.0051, 0.0029, 0.0029, 0.0017, 0.0025, 0.0044,\n",
            "         0.0035, 0.0042, 0.0042, 0.0039, 0.0019, 0.0017, 0.0037, 0.0025, 0.0025,\n",
            "         0.0046, 0.0035, 0.0049, 0.0056, 0.0028, 0.0027, 0.0014, 0.0022, 0.0036,\n",
            "         0.0028, 0.0043, 0.0019, 0.0026, 0.0016, 0.0023, 0.0029, 0.0025, 0.0034,\n",
            "         0.0046, 0.0013, 0.0024, 0.0041, 0.0019, 0.0024, 0.0042, 0.0032, 0.0016,\n",
            "         0.0038, 0.0038, 0.0017, 0.0038, 0.0027, 0.0014, 0.0032, 0.0027, 0.0027,\n",
            "         0.0042, 0.0023, 0.0015, 0.0015, 0.0023, 0.0030, 0.0034, 0.0098, 0.0033,\n",
            "         0.0065, 0.0019, 0.0037, 0.0036, 0.0017, 0.0050, 0.0028, 0.0014, 0.0020,\n",
            "         0.0047, 0.0032, 0.0026, 0.0025, 0.0047, 0.0014, 0.0020, 0.0040, 0.0016,\n",
            "         0.0027, 0.0040, 0.0040, 0.0016, 0.0037, 0.0041, 0.0082, 0.0036, 0.0056,\n",
            "         0.0023, 0.0030, 0.0027, 0.0041, 0.0020, 0.0014, 0.0041, 0.0057, 0.0034,\n",
            "         0.0069, 0.0048, 0.0056, 0.0036, 0.0017, 0.0039, 0.0033, 0.0028, 0.0028,\n",
            "         0.0041, 0.0022, 0.0027, 0.0046, 0.0033, 0.0037, 0.0033, 0.0036, 0.0033,\n",
            "         0.0025, 0.0033, 0.0049, 0.0065, 0.0026, 0.0047, 0.0030, 0.0019, 0.0027,\n",
            "         0.0025, 0.0046, 0.0025, 0.0068, 0.0029, 0.0030, 0.0029, 0.0024, 0.0032,\n",
            "         0.0021, 0.0035, 0.0023, 0.0029, 0.0023, 0.0024, 0.0039, 0.0039, 0.0048,\n",
            "         0.0035, 0.0017, 0.0018, 0.0022, 0.0036, 0.0047, 0.0031, 0.0024, 0.0035,\n",
            "         0.0054, 0.0031, 0.0019, 0.0055, 0.0019, 0.0029, 0.0040, 0.0034, 0.0032,\n",
            "         0.0025, 0.0039, 0.0031, 0.0027, 0.0048, 0.0020, 0.0020, 0.0012, 0.0041,\n",
            "         0.0059, 0.0022, 0.0039, 0.0048, 0.0074, 0.0039, 0.0059, 0.0026, 0.0031,\n",
            "         0.0024, 0.0022, 0.0022, 0.0039, 0.0034, 0.0047, 0.0014, 0.0087, 0.0026,\n",
            "         0.0016, 0.0014, 0.0044, 0.0074, 0.0030, 0.0030, 0.0040, 0.0050, 0.0016,\n",
            "         0.0026, 0.0053, 0.0044, 0.0029, 0.0023, 0.0030, 0.0026, 0.0039, 0.0028,\n",
            "         0.0071, 0.0044, 0.0025, 0.0039, 0.0049, 0.0037, 0.0016, 0.0026, 0.0031,\n",
            "         0.0045, 0.0029, 0.0035, 0.0026, 0.0039, 0.0031, 0.0048, 0.0028, 0.0015,\n",
            "         0.0027, 0.0023, 0.0025, 0.0031, 0.0024, 0.0021, 0.0028]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "old 5 presidency 253\n",
            "tensor([[0.0016, 0.0040, 0.0022, 0.0033, 0.0029, 0.0051, 0.0051, 0.0034, 0.0020,\n",
            "         0.0033, 0.0054, 0.0024, 0.0022, 0.0112, 0.0023, 0.0064, 0.0033, 0.0010,\n",
            "         0.0029, 0.0019, 0.0022, 0.0031, 0.0036, 0.0027, 0.0043, 0.0073, 0.0015,\n",
            "         0.0024, 0.0019, 0.0044, 0.0023, 0.0036, 0.0021, 0.0048, 0.0082, 0.0017,\n",
            "         0.0029, 0.0046, 0.0020, 0.0056, 0.0020, 0.0024, 0.0021, 0.0053, 0.0024,\n",
            "         0.0038, 0.0039, 0.0053, 0.0025, 0.0030, 0.0019, 0.0035, 0.0023, 0.0031,\n",
            "         0.0036, 0.0031, 0.0056, 0.0024, 0.0050, 0.0046, 0.0035, 0.0062, 0.0015,\n",
            "         0.0047, 0.0055, 0.0045, 0.0021, 0.0028, 0.0055, 0.0090, 0.0042, 0.0014,\n",
            "         0.0023, 0.0014, 0.0029, 0.0025, 0.0027, 0.0026, 0.0032, 0.0016, 0.0016,\n",
            "         0.0025, 0.0033, 0.0019, 0.0028, 0.0055, 0.0041, 0.0017, 0.0042, 0.0029,\n",
            "         0.0024, 0.0038, 0.0013, 0.0061, 0.0052, 0.0019, 0.0037, 0.0040, 0.0016,\n",
            "         0.0011, 0.0043, 0.0049, 0.0015, 0.0037, 0.0028, 0.0036, 0.0034, 0.0012,\n",
            "         0.0060, 0.0051, 0.0040, 0.0035, 0.0030, 0.0038, 0.0048, 0.0024, 0.0023,\n",
            "         0.0031, 0.0019, 0.0040, 0.0043, 0.0028, 0.0034, 0.0048, 0.0043, 0.0083,\n",
            "         0.0056, 0.0039, 0.0024, 0.0038, 0.0047, 0.0042, 0.0025, 0.0032, 0.0032,\n",
            "         0.0021, 0.0047, 0.0024, 0.0034, 0.0019, 0.0023, 0.0051, 0.0024, 0.0044,\n",
            "         0.0035, 0.0029, 0.0048, 0.0021, 0.0034, 0.0020, 0.0054, 0.0052, 0.0016,\n",
            "         0.0015, 0.0025, 0.0030, 0.0038, 0.0038, 0.0011, 0.0067, 0.0045, 0.0021,\n",
            "         0.0037, 0.0039, 0.0019, 0.0035, 0.0029, 0.0059, 0.0042, 0.0041, 0.0042,\n",
            "         0.0015, 0.0038, 0.0022, 0.0028, 0.0022, 0.0022, 0.0046, 0.0025, 0.0037,\n",
            "         0.0046, 0.0011, 0.0025, 0.0056, 0.0044, 0.0028, 0.0034, 0.0027, 0.0028,\n",
            "         0.0039, 0.0026, 0.0034, 0.0043, 0.0020, 0.0017, 0.0035, 0.0030, 0.0034,\n",
            "         0.0034, 0.0035, 0.0024, 0.0018, 0.0043, 0.0028, 0.0024, 0.0033, 0.0036,\n",
            "         0.0026, 0.0028, 0.0030, 0.0014, 0.0036, 0.0053, 0.0034, 0.0014, 0.0024,\n",
            "         0.0034, 0.0072, 0.0031, 0.0020, 0.0045, 0.0025, 0.0047, 0.0031, 0.0021,\n",
            "         0.0037, 0.0019, 0.0023, 0.0017, 0.0012, 0.0060, 0.0042, 0.0043, 0.0026,\n",
            "         0.0027, 0.0023, 0.0041, 0.0015, 0.0035, 0.0034, 0.0022, 0.0071, 0.0036,\n",
            "         0.0036, 0.0042, 0.0053, 0.0029, 0.0034, 0.0023, 0.0041, 0.0045, 0.0027,\n",
            "         0.0051, 0.0050, 0.0021, 0.0060, 0.0038, 0.0024, 0.0053, 0.0018, 0.0032,\n",
            "         0.0023, 0.0054, 0.0017, 0.0045, 0.0022, 0.0018, 0.0054, 0.0036, 0.0031,\n",
            "         0.0075, 0.0020, 0.0062, 0.0027, 0.0049, 0.0039, 0.0020, 0.0053, 0.0019,\n",
            "         0.0027, 0.0027, 0.0025, 0.0025, 0.0018, 0.0018, 0.0033, 0.0026, 0.0040,\n",
            "         0.0027, 0.0022, 0.0042, 0.0024, 0.0013, 0.0036, 0.0013]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "single 968 back 411\n",
            "tensor([[0.0025, 0.0035, 0.0017, 0.0045, 0.0068, 0.0037, 0.0044, 0.0042, 0.0031,\n",
            "         0.0034, 0.0032, 0.0040, 0.0057, 0.0039, 0.0034, 0.0054, 0.0032, 0.0013,\n",
            "         0.0026, 0.0015, 0.0053, 0.0065, 0.0038, 0.0034, 0.0031, 0.0031, 0.0040,\n",
            "         0.0017, 0.0029, 0.0020, 0.0023, 0.0031, 0.0046, 0.0035, 0.0046, 0.0016,\n",
            "         0.0027, 0.0034, 0.0013, 0.0077, 0.0029, 0.0030, 0.0022, 0.0043, 0.0031,\n",
            "         0.0033, 0.0019, 0.0031, 0.0020, 0.0034, 0.0017, 0.0021, 0.0027, 0.0033,\n",
            "         0.0020, 0.0048, 0.0043, 0.0055, 0.0047, 0.0040, 0.0026, 0.0023, 0.0049,\n",
            "         0.0021, 0.0027, 0.0020, 0.0011, 0.0019, 0.0045, 0.0014, 0.0045, 0.0018,\n",
            "         0.0028, 0.0032, 0.0061, 0.0028, 0.0042, 0.0034, 0.0030, 0.0024, 0.0027,\n",
            "         0.0021, 0.0017, 0.0046, 0.0033, 0.0042, 0.0069, 0.0025, 0.0040, 0.0020,\n",
            "         0.0035, 0.0049, 0.0017, 0.0014, 0.0041, 0.0056, 0.0018, 0.0068, 0.0033,\n",
            "         0.0024, 0.0018, 0.0075, 0.0036, 0.0023, 0.0019, 0.0029, 0.0047, 0.0025,\n",
            "         0.0017, 0.0017, 0.0027, 0.0030, 0.0032, 0.0021, 0.0031, 0.0043, 0.0039,\n",
            "         0.0040, 0.0023, 0.0054, 0.0032, 0.0033, 0.0051, 0.0038, 0.0035, 0.0024,\n",
            "         0.0028, 0.0059, 0.0048, 0.0029, 0.0029, 0.0036, 0.0029, 0.0030, 0.0017,\n",
            "         0.0033, 0.0024, 0.0021, 0.0044, 0.0076, 0.0035, 0.0037, 0.0015, 0.0019,\n",
            "         0.0033, 0.0032, 0.0079, 0.0049, 0.0070, 0.0025, 0.0104, 0.0039, 0.0051,\n",
            "         0.0020, 0.0051, 0.0025, 0.0025, 0.0051, 0.0037, 0.0030, 0.0024, 0.0063,\n",
            "         0.0024, 0.0014, 0.0030, 0.0041, 0.0059, 0.0039, 0.0051, 0.0027, 0.0020,\n",
            "         0.0048, 0.0021, 0.0028, 0.0022, 0.0021, 0.0049, 0.0044, 0.0047, 0.0023,\n",
            "         0.0054, 0.0049, 0.0036, 0.0026, 0.0024, 0.0026, 0.0020, 0.0021, 0.0036,\n",
            "         0.0018, 0.0047, 0.0033, 0.0025, 0.0027, 0.0057, 0.0037, 0.0036, 0.0026,\n",
            "         0.0044, 0.0019, 0.0040, 0.0040, 0.0061, 0.0029, 0.0013, 0.0035, 0.0024,\n",
            "         0.0032, 0.0038, 0.0034, 0.0046, 0.0027, 0.0016, 0.0040, 0.0029, 0.0041,\n",
            "         0.0048, 0.0037, 0.0046, 0.0037, 0.0031, 0.0045, 0.0044, 0.0013, 0.0033,\n",
            "         0.0016, 0.0061, 0.0031, 0.0041, 0.0040, 0.0028, 0.0019, 0.0029, 0.0022,\n",
            "         0.0028, 0.0035, 0.0035, 0.0029, 0.0014, 0.0070, 0.0029, 0.0026, 0.0032,\n",
            "         0.0042, 0.0051, 0.0028, 0.0028, 0.0023, 0.0030, 0.0015, 0.0039, 0.0016,\n",
            "         0.0020, 0.0035, 0.0053, 0.0050, 0.0028, 0.0019, 0.0040, 0.0028, 0.0033,\n",
            "         0.0019, 0.0025, 0.0025, 0.0035, 0.0018, 0.0034, 0.0022, 0.0052, 0.0014,\n",
            "         0.0016, 0.0032, 0.0028, 0.0035, 0.0030, 0.0028, 0.0017, 0.0010, 0.0039,\n",
            "         0.0033, 0.0019, 0.0034, 0.0044, 0.0023, 0.0034, 0.0051, 0.0075, 0.0015,\n",
            "         0.0013, 0.0031, 0.0094, 0.0051, 0.0017, 0.0018, 0.0022]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqyMLV6ZXivn",
        "outputId": "453fcf5e-e2b3-47e1-fd23-08fa8fa46591"
      },
      "source": [
        "output_indices, output_sentence, input_sentence"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2,\n",
              "  21,\n",
              "  29,\n",
              "  4,\n",
              "  5,\n",
              "  968,\n",
              "  560,\n",
              "  8,\n",
              "  76,\n",
              "  95,\n",
              "  99,\n",
              "  27,\n",
              "  156,\n",
              "  17,\n",
              "  319,\n",
              "  110,\n",
              "  176,\n",
              "  1260,\n",
              "  217,\n",
              "  332,\n",
              "  1261,\n",
              "  374,\n",
              "  6,\n",
              "  257,\n",
              "  19,\n",
              "  1],\n",
              " 'i am a year old single male is it too late for me to find love would women be put off by my age ?',\n",
              " 'i am a year old man . is it too late to find love . would my age put women off ?')"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdDT4RrbYEZx"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMKJsJ3GYH2m"
      },
      "source": [
        "# from __future__ import unicode_literals, print_function, division\n",
        "# from io import open\n",
        "# import unicodedata\n",
        "# import string\n",
        "# import re\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch import optim\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_2OIu6iYKUt",
        "outputId": "ca852436-ce17-4877-f53b-126db4e9da9a"
      },
      "source": [
        "# device"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-to1tUHYLnx",
        "outputId": "1855bcff-9f10-4a17-d5f0-f17343d0bc4a"
      },
      "source": [
        "# !wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "# !unzip data.zip"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-25 16:04:25--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.226.52.90, 13.226.52.51, 13.226.52.128, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.226.52.90|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-11-25 16:04:25 (21.3 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TeplhycYMWv"
      },
      "source": [
        "# SOS_token = 0\n",
        "# EOS_token = 1\n",
        "\n",
        "\n",
        "# class Lang:\n",
        "#     def __init__(self, name):\n",
        "#         self.name = name\n",
        "#         self.word2index = {}\n",
        "#         self.word2count = {}\n",
        "#         self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "#         self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "#     def addSentence(self, sentence):\n",
        "#         for word in sentence.split(' '):\n",
        "#             self.addWord(word)\n",
        "\n",
        "#     def addWord(self, word):\n",
        "#         if word not in self.word2index:\n",
        "#             self.word2index[word] = self.n_words\n",
        "#             self.word2count[word] = 1\n",
        "#             self.index2word[self.n_words] = word\n",
        "#             self.n_words += 1\n",
        "#         else:\n",
        "#             self.word2count[word] += 1"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0inG08EYOKf"
      },
      "source": [
        "# # Turn a Unicode string to plain ASCII, thanks to\n",
        "# # https://stackoverflow.com/a/518232/2809427\n",
        "# def unicodeToAscii(s):\n",
        "#     return ''.join(\n",
        "#         c for c in unicodedata.normalize('NFD', s)\n",
        "#         if unicodedata.category(c) != 'Mn'\n",
        "#     )\n",
        "\n",
        "# # Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "# def normalizeString(s):\n",
        "#     s = unicodeToAscii(s.lower().strip())\n",
        "#     s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "#     return s"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5os4B6QYPZ1"
      },
      "source": [
        "# def readLangs(lang1, lang2, reverse=False):\n",
        "#     print(\"Reading lines...\")\n",
        "\n",
        "#     # Read the file and split into lines\n",
        "#     lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "\n",
        "#     # Split every line into pairs and normalize\n",
        "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "#     # Reverse pairs, make Lang instances\n",
        "#     if reverse:\n",
        "#         pairs = [list(reversed(p)) for p in pairs]\n",
        "#         input_lang = Lang(lang2)\n",
        "#         output_lang = Lang(lang1)\n",
        "#     else:\n",
        "#         input_lang = Lang(lang1)\n",
        "#         output_lang = Lang(lang2)\n",
        "\n",
        "#     return input_lang, output_lang, pairs"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg2mGsUGYRI1"
      },
      "source": [
        "MAX_LENGTH = 295\n",
        "\n",
        "# eng_prefixes = (\n",
        "#     \"i am \", \"i m \",\n",
        "#     \"he is\", \"he s \",\n",
        "#     \"she is\", \"she s \",\n",
        "#     \"you are\", \"you re \",\n",
        "#     \"we are\", \"we re \",\n",
        "#     \"they are\", \"they re \"\n",
        "# )\n",
        "\n",
        "\n",
        "# def filterPair(p):\n",
        "#     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "#         len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "#         p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "# def filterPairs(pairs):\n",
        "#     return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOYzEWwWYSfj",
        "outputId": "ce5b2d14-3197-4d76-8852-d9ef22a4bf8c"
      },
      "source": [
        "# def prepareData(lang1, lang2, reverse=False):\n",
        "#     input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "#     print(\"Read %s sentence pairs\" % len(pairs))\n",
        "#     pairs = filterPairs(pairs)\n",
        "#     print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "#     print(\"Counting words...\")\n",
        "#     for pair in pairs:\n",
        "#         input_lang.addSentence(pair[0])\n",
        "#         output_lang.addSentence(pair[1])\n",
        "#     print(\"Counted words:\")\n",
        "#     print(input_lang.name, input_lang.n_words)\n",
        "#     print(output_lang.name, output_lang.n_words)\n",
        "#     return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "# input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "# print(random.choice(pairs))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "['c est une fille mais elle est brave .', 'she s a girl but she s brave .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq_rtqvzYTgG"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgKx-hHuYspE"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wadJ6xpyZCF9"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyeUE5oaZG3d"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS8uKk2FZo44"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq0C4hNbZqIm"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Uu4NaExZuJ9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8waPFLdZvf7"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud8lBnmqZwbl"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGPbkKkLZxtx"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ipQPS5LZy-v"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIMs86i5-d_-",
        "outputId": "855f0984-19f0-4161-e328-314f8e6a4972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> il est un peu naif .\n",
            "= he s a bit naive .\n",
            "< he is a a a a a a a a\n",
            "\n",
            "> tu es sournois .\n",
            "= you re sneaky .\n",
            "< you re . . . <EOS>\n",
            "\n",
            "> je vais aller rejoindre les bras de morphee .\n",
            "= i am going to sleep .\n",
            "< i m . . . . . . . .\n",
            "\n",
            "> nous sommes depourvues de talent .\n",
            "= we re untalented .\n",
            "< we re not . <EOS>\n",
            "\n",
            "> nous sommes enfin seules .\n",
            "= we re finally alone .\n",
            "< we re . . . . . . . .\n",
            "\n",
            "> c est une beaute .\n",
            "= she s a cutie .\n",
            "< he s a . <EOS>\n",
            "\n",
            "> on m utilise .\n",
            "= i m being used .\n",
            "< i m . . <EOS>\n",
            "\n",
            "> je suis honnete .\n",
            "= i m honest .\n",
            "< i m . <EOS>\n",
            "\n",
            "> je suis satisfait .\n",
            "= i m satisfied .\n",
            "< i m . <EOS>\n",
            "\n",
            "> je suis vraiment desole .\n",
            "= i am truly sorry .\n",
            "< i m . . <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWlpTWHWRW1o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}